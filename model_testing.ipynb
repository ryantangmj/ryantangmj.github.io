{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 1. Import Libraries\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from scipy.stats import linregress\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2. Define State Abbreviation Mapping\n",
    "# ==============================================\n",
    "\n",
    "# Mapping of state abbreviations to full state names in lowercase\n",
    "state_abbrev_to_name = {\n",
    "    'al': 'alabama', 'ak': 'alaska', 'az': 'arizona', 'ar': 'arkansas',\n",
    "    'ca': 'california', 'co': 'colorado', 'ct': 'connecticut', 'de': 'delaware',\n",
    "    'fl': 'florida', 'ga': 'georgia', 'hi': 'hawaii', 'id': 'idaho',\n",
    "    'il': 'illinois', 'in': 'indiana', 'ia': 'iowa', 'ks': 'kansas',\n",
    "    'ky': 'kentucky', 'la': 'louisiana', 'me': 'maine', 'md': 'maryland',\n",
    "    'ma': 'massachusetts', 'mi': 'michigan', 'mn': 'minnesota', 'ms': 'mississippi',\n",
    "    'mo': 'missouri', 'mt': 'montana', 'ne': 'nebraska', 'nv': 'nevada',\n",
    "    'nh': 'new hampshire', 'nj': 'new jersey', 'nm': 'new mexico', 'ny': 'new york',\n",
    "    'nc': 'north carolina', 'nd': 'north dakota', 'oh': 'ohio', 'ok': 'oklahoma',\n",
    "    'or': 'oregon', 'pa': 'pennsylvania', 'ri': 'rhode island', 'sc': 'south carolina',\n",
    "    'sd': 'south dakota', 'tn': 'tennessee', 'tx': 'texas', 'ut': 'utah',\n",
    "    'vt': 'vermont', 'va': 'virginia', 'wa': 'washington', 'wv': 'west virginia',\n",
    "    'wi': 'wisconsin', 'wy': 'wyoming', 'dc': 'district of columbia',\n",
    "    'as': 'american samoa', 'gu': 'guam', 'mp': 'northern mariana islands',\n",
    "    'pr': 'puerto rico', 'vi': 'virgin islands', 'us': 'united states'\n",
    "}\n",
    "\n",
    "# Define territories and non-continental states to exclude\n",
    "exclude_states = [\n",
    "    'puerto rico', 'guam', 'virgin islands', 'american samoa',\n",
    "    'northern mariana islands', 'alaska', 'hawaii', 'united states'\n",
    "]\n",
    "\n",
    "exclude_states_lower = [state.lower() for state in exclude_states]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 3. Load and Process Cargo Fraud Data\n",
    "# ==============================================\n",
    "\n",
    "# Load the cargo fraud data\n",
    "cargo_fraud = pd.read_csv('cargo_fraud_only.csv')\n",
    "\n",
    "# Check if 'data_year' column exists\n",
    "if 'data_year' in cargo_fraud.columns:\n",
    "    # Use 'data_year' as 'Year'\n",
    "    cargo_fraud['Year'] = cargo_fraud['data_year'].astype(int)\n",
    "else:\n",
    "    # Extract 'Year' from 'date_recovered' or another date column\n",
    "    cargo_fraud['date_recovered'] = pd.to_datetime(cargo_fraud['date_recovered'], errors='coerce')\n",
    "    cargo_fraud['Year'] = cargo_fraud['date_recovered'].dt.year\n",
    "    cargo_fraud = cargo_fraud.dropna(subset=['Year'])\n",
    "    cargo_fraud['Year'] = cargo_fraud['Year'].astype(int)\n",
    "\n",
    "# Ensure the 'State' column exists and matches the 'State' in other datasets\n",
    "if 'state_name' in cargo_fraud.columns:\n",
    "    cargo_fraud['State'] = cargo_fraud['state_name'].str.lower().str.strip()\n",
    "elif 'state_abbr' in cargo_fraud.columns:\n",
    "    # Map state abbreviations to full state names\n",
    "    cargo_fraud['State'] = cargo_fraud['state_abbr'].str.lower().map(state_abbrev_to_name)\n",
    "else:\n",
    "    raise KeyError(\"No 'state_name' or 'state_abbr' column found in cargo_fraud DataFrame.\")\n",
    "\n",
    "# Exclude rows with missing 'State' or 'Year'\n",
    "cargo_fraud = cargo_fraud.dropna(subset=['State', 'Year'])\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "cargo_fraud = cargo_fraud[~cargo_fraud['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Display the processed cargo fraud data\n",
    "print(\"\\nProcessed Cargo Fraud Data:\")\n",
    "print(cargo_fraud[['State', 'Year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 4. Load and Process HPI Data\n",
    "# ==============================================\n",
    "\n",
    "# Load HPI data\n",
    "hpi_data_url = \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/main/hpi_by_state.csv\"\n",
    "hpi_data = pd.read_csv(hpi_data_url)\n",
    "\n",
    "# Keep relevant columns\n",
    "hpi_data = hpi_data[[\"State\", \"Year\", \"HPI\"]]\n",
    "\n",
    "# Filter years between 2012 and 2022\n",
    "hpi_data = hpi_data[(hpi_data[\"Year\"] >= 2012) & (hpi_data[\"Year\"] <= 2022)].reset_index(drop=True)\n",
    "\n",
    "# Standardize 'State' names\n",
    "hpi_data['State'] = hpi_data['State'].str.lower().str.strip()\n",
    "\n",
    "# Map state abbreviations to full names if necessary\n",
    "if hpi_data['State'].str.len().max() == 2:\n",
    "    hpi_data['State'] = hpi_data['State'].map(state_abbrev_to_name)\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "hpi_data = hpi_data[~hpi_data['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "hpi_data['Year'] = hpi_data['Year'].astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nProcessed HPI Data:\")\n",
    "print(hpi_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 5. Load and Process Poverty Data\n",
    "# ==============================================\n",
    "\n",
    "# Load poverty data\n",
    "poverty_data = pd.read_csv('poverty_data.csv')  # Replace with the actual file path\n",
    "\n",
    "# Reshape poverty data to long format\n",
    "id_vars = ['State']\n",
    "value_vars = [col for col in poverty_data.columns if col != 'State']\n",
    "\n",
    "poverty_long = pd.melt(poverty_data, id_vars=id_vars, value_vars=value_vars,\n",
    "                       var_name='Variable', value_name='Value')\n",
    "\n",
    "# Extract 'Year' from the 'Variable' column\n",
    "poverty_long['Year'] = poverty_long['Variable'].str.extract('(\\d{4})', expand=False).astype(int)\n",
    "poverty_long['Variable_Name'] = poverty_long['Variable'].str.replace(' \\d{4}', '', regex=True).str.strip()\n",
    "\n",
    "# Pivot the data to have one row per 'State' and 'Year'\n",
    "poverty_pivot = poverty_long.pivot_table(index=['State', 'Year'], columns='Variable_Name', values='Value', aggfunc='first').reset_index()\n",
    "poverty_pivot.columns.name = None\n",
    "\n",
    "# Standardize 'State' names\n",
    "poverty_pivot['State'] = poverty_pivot['State'].str.lower().str.strip()\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = ['Total population', 'Number in poverty', 'Percentage poverty']\n",
    "for col in numeric_cols:\n",
    "    poverty_pivot[col] = poverty_pivot[col].astype(str).replace('nan', '').str.replace(',', '').str.replace('%', '')\n",
    "    poverty_pivot[col] = poverty_pivot[col].replace('', pd.NA)\n",
    "    poverty_pivot[col] = poverty_pivot[col].astype(float)\n",
    "\n",
    "# Rename columns for clarity\n",
    "poverty_pivot.rename(columns={\n",
    "    'Total population': 'Total_Population',\n",
    "    'Number in poverty': 'Number_in_Poverty',\n",
    "    'Percentage poverty': 'Poverty_Rate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "poverty_pivot = poverty_pivot[~poverty_pivot['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nProcessed Poverty Data:\")\n",
    "print(poverty_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 6. Load and Process Homelessness Data\n",
    "# ==============================================\n",
    "\n",
    "# Load homelessness data\n",
    "homelessness_data_url = \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/main/homeless_data.csv\"\n",
    "homelessness_data = pd.read_csv(homelessness_data_url)\n",
    "\n",
    "# Map state abbreviations to full state names\n",
    "homelessness_data['State'] = homelessness_data['State'].str.lower().map(state_abbrev_to_name)\n",
    "\n",
    "# Exclude rows with missing 'State'\n",
    "homelessness_data = homelessness_data.dropna(subset=['State'])\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "homelessness_data = homelessness_data[~homelessness_data['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Rename columns for years (Assuming columns have year information)\n",
    "new_column_names = {\n",
    "    col: col.split(', ')[-1].split('-')[0] for col in homelessness_data.columns if 'Change in Total Homelessness' in col\n",
    "}\n",
    "homelessness_data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df_homelessness = pd.melt(homelessness_data, id_vars=['State'], var_name='Year', value_name='homeless_rate_change')\n",
    "\n",
    "# Clean 'homeless_rate_change' column\n",
    "df_homelessness['homeless_rate_change'] = df_homelessness['homeless_rate_change'].replace(' ', np.nan)\n",
    "df_homelessness = df_homelessness.dropna(subset=['homeless_rate_change'])\n",
    "df_homelessness['homeless_rate_change'] = df_homelessness['homeless_rate_change'].str.replace('%', '').astype(float)\n",
    "df_homelessness['Year'] = df_homelessness['Year'].astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nProcessed Homelessness Data:\")\n",
    "print(df_homelessness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 7. Load and Process Education Data\n",
    "# ==============================================\n",
    "\n",
    "# Load education data\n",
    "education_data = pd.read_csv('education.csv')  # Replace with the actual file path\n",
    "\n",
    "# Display the first few rows to verify the structure\n",
    "print(\"\\nInitial Education Data:\")\n",
    "print(education_data.head())\n",
    "\n",
    "# Identify columns related to education metrics for years 2012-2022\n",
    "education_cols = [col for col in education_data.columns if any(str(year) in col for year in range(2012, 2023))]\n",
    "\n",
    "# Keep 'State' and the identified education columns\n",
    "education_data = education_data[['State'] + education_cols]\n",
    "\n",
    "# Melt the data to long format\n",
    "education_long = pd.melt(\n",
    "    education_data,\n",
    "    id_vars=['State'],\n",
    "    value_vars=education_cols,\n",
    "    var_name='Variable',\n",
    "    value_name='Value'\n",
    ")\n",
    "\n",
    "# Extract 'Year' and 'Education_Variable' from the 'Variable' column\n",
    "education_long['Year'] = education_long['Variable'].str.extract('(\\d{4})', expand=False).astype(int)\n",
    "education_long['Education_Variable'] = education_long['Variable'].str.replace(' \\d{4}', '', regex=True).str.strip()\n",
    "\n",
    "# Drop rows where 'Year' is NaN\n",
    "education_long = education_long.dropna(subset=['Year'])\n",
    "\n",
    "# Pivot the data to have one row per 'State' and 'Year'\n",
    "education_pivot = education_long.pivot_table(\n",
    "    index=['State', 'Year'],\n",
    "    columns='Education_Variable',\n",
    "    values='Value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the columns\n",
    "education_pivot.columns.name = None\n",
    "\n",
    "# Standardize 'State' names\n",
    "education_pivot['State'] = education_pivot['State'].str.lower().str.strip()\n",
    "\n",
    "# Map state abbreviations to full state names if necessary\n",
    "if education_pivot['State'].str.len().max() == 2:\n",
    "    education_pivot['State'] = education_pivot['State'].map(state_abbrev_to_name)\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "education_pivot = education_pivot[~education_pivot['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = [col for col in education_pivot.columns if col not in ['State', 'Year']]\n",
    "for col in numeric_cols:\n",
    "    # Convert to string and remove commas and percent signs\n",
    "    education_pivot[col] = education_pivot[col].astype(str).str.replace(',', '').str.replace('%', '')\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    education_pivot[col] = pd.to_numeric(education_pivot[col], errors='coerce')\n",
    "\n",
    "# Rename columns for clarity (Adjust based on actual column names)\n",
    "# Example:\n",
    "# education_pivot.rename(columns={\n",
    "#     \"Percentage with Bachelor's Degree\": 'Bachelor_Degree_Rate'\n",
    "# }, inplace=True)\n",
    "\n",
    "# Exclude rows with missing 'State' or 'Year' after mapping\n",
    "education_pivot = education_pivot.dropna(subset=['State', 'Year'])\n",
    "\n",
    "# Display the first few rows of the processed education data\n",
    "print(\"\\nProcessed Education Data (2012-2022):\")\n",
    "print(education_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 8. Load and Process Unemployment Data\n",
    "# ==============================================\n",
    "\n",
    "# Load the unemployment data\n",
    "unemployment_data = pd.read_csv('Unemployment.csv')  # Replace with the actual file path\n",
    "\n",
    "# Identify columns that contain 'Unemployment_rate'\n",
    "unemployment_rate_cols = [col for col in unemployment_data.columns if 'Unemployment_rate' in col]\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "unemployment_long = pd.melt(\n",
    "    unemployment_data,\n",
    "    id_vars=['State', 'Area_Name'],\n",
    "    value_vars=unemployment_rate_cols,\n",
    "    var_name='Year',\n",
    "    value_name='Unemployment_Rate'\n",
    ")\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "unemployment_long['Year'] = unemployment_long['Year'].str.extract('Unemployment_rate_(\\d+)', expand=False).astype(int)\n",
    "\n",
    "# Convert 'Unemployment_Rate' to numeric\n",
    "unemployment_long['Unemployment_Rate'] = pd.to_numeric(unemployment_long['Unemployment_Rate'], errors='coerce')\n",
    "\n",
    "# Standardize 'State' names\n",
    "unemployment_long['State'] = unemployment_long['State'].str.lower().str.strip()\n",
    "\n",
    "# Map state abbreviations to full state names if necessary\n",
    "if unemployment_long['State'].str.len().max() == 2:\n",
    "    unemployment_long['State'] = unemployment_long['State'].map(state_abbrev_to_name)\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "unemployment_long = unemployment_long[~unemployment_long['State'].isin(exclude_states_lower)]\n",
    "\n",
    "# Aggregate at the state level by taking the mean unemployment rate for each state and year\n",
    "state_unemployment = unemployment_long.groupby(['State', 'Year'])['Unemployment_Rate'].mean().reset_index()\n",
    "\n",
    "# Display the processed unemployment data\n",
    "print(\"\\nProcessed Unemployment Data:\")\n",
    "print(state_unemployment.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 9. Merge All Datasets\n",
    "# ==============================================\n",
    "\n",
    "# Aggregate the number of fraud cases per state and year\n",
    "fraud_counts = cargo_fraud.groupby(['State', 'Year']).size().reset_index(name='Fraud_Count')\n",
    "\n",
    "# Ensure 'State' and 'Year' are of appropriate types\n",
    "fraud_counts['State'] = fraud_counts['State'].str.lower().str.strip()\n",
    "fraud_counts['Year'] = fraud_counts['Year'].astype(int)\n",
    "\n",
    "# Define a function to prepare dataframes for merging\n",
    "def prepare_dataframe(df, state_col='State', year_col='Year'):\n",
    "    df[state_col] = df[state_col].str.lower().str.strip()\n",
    "    df[year_col] = df[year_col].astype(int)\n",
    "    return df\n",
    "\n",
    "# Prepare all datasets\n",
    "fraud_counts = prepare_dataframe(fraud_counts)\n",
    "poverty_pivot = prepare_dataframe(poverty_pivot)\n",
    "education_pivot = prepare_dataframe(education_pivot)\n",
    "hpi_data = prepare_dataframe(hpi_data)\n",
    "state_unemployment = prepare_dataframe(state_unemployment)\n",
    "\n",
    "# Merge fraud_counts with poverty_pivot\n",
    "merged_data = pd.merge(fraud_counts, poverty_pivot, on=['State', 'Year'], how='left')\n",
    "print(\"\\nAfter merging with poverty_pivot:\", merged_data.shape)\n",
    "print(merged_data.head())\n",
    "\n",
    "# Merge with education_pivot\n",
    "merged_data = pd.merge(merged_data, education_pivot, on=['State', 'Year'], how='left')\n",
    "print(\"After merging with education_pivot:\", merged_data.shape)\n",
    "print(merged_data.head())\n",
    "\n",
    "# Merge with hpi_data\n",
    "merged_data = pd.merge(merged_data, hpi_data, on=['State', 'Year'], how='left')\n",
    "print(\"After merging with hpi_data:\", merged_data.shape)\n",
    "print(merged_data.head())\n",
    "\n",
    "# Merge with state_unemployment\n",
    "merged_data = pd.merge(merged_data, state_unemployment, on=['State', 'Year'], how='left')\n",
    "print(\"After merging with state_unemployment:\", merged_data.shape)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 10. Handle Missing Values\n",
    "# ==============================================\n",
    "\n",
    "# Display the count of missing values in each column\n",
    "print(\"\\nMissing values in merged_data:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Check if merged_data is empty\n",
    "if merged_data.empty:\n",
    "    print(\"Error: 'merged_data' is empty. Please check the merging steps.\")\n",
    "else:\n",
    "    # Define critical columns for analysis\n",
    "    critical_columns = ['Fraud_Count', 'Poverty_Rate', 'HPI', 'Unemployment_Rate']\n",
    "\n",
    "    # Initialize imputers\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Impute 'Poverty_Rate' and 'HPI' with mean\n",
    "    for col in ['Poverty_Rate', 'HPI']:\n",
    "        if col in merged_data.columns:\n",
    "            merged_data[col] = mean_imputer.fit_transform(merged_data[[col]])\n",
    "            print(f\"Imputed missing values in '{col}' with mean.\")\n",
    "        else:\n",
    "            print(f\"Warning: '{col}' column not found in merged_data.\")\n",
    "\n",
    "    # Impute 'Unemployment_Rate' with median\n",
    "    col = 'Unemployment_Rate'\n",
    "    if col in merged_data.columns:\n",
    "        merged_data[col] = median_imputer.fit_transform(merged_data[[col]])\n",
    "        print(f\"Imputed missing values in '{col}' with median.\")\n",
    "    else:\n",
    "        print(f\"Warning: '{col}' column not found in merged_data.\")\n",
    "\n",
    "    # Identify columns with more than 50% missing values\n",
    "    threshold = 0.5\n",
    "    missing_percent = merged_data.isnull().mean()\n",
    "    columns_to_drop = missing_percent[missing_percent > threshold].index.tolist()\n",
    "\n",
    "    print(\"\\nColumns to drop due to high missingness (>50% missing):\")\n",
    "    print(columns_to_drop)\n",
    "\n",
    "    # Drop these columns\n",
    "    merged_data_clean = merged_data.drop(columns=columns_to_drop)\n",
    "    print(\"\\nDropped columns with high missingness.\")\n",
    "\n",
    "    # Verify the number of rows after dropping\n",
    "    print(f\"\\nNumber of rows before dropping high missingness columns: {merged_data.shape[0]}\")\n",
    "    print(f\"Number of rows after dropping high missingness columns: {merged_data_clean.shape[0]}\")\n",
    "\n",
    "    # Final check for missing values\n",
    "    print(\"\\nMissing values in merged_data_clean after handling:\")\n",
    "    print(merged_data_clean.isnull().sum())\n",
    "\n",
    "    # Optional: Impute remaining low missingness columns if necessary\n",
    "    # In your case, 'Number_in_Poverty' and 'Total_Population' have 5 missing values each\n",
    "    remaining_imputer = SimpleImputer(strategy='mean')\n",
    "    for col in ['Number_in_Poverty', 'Total_Population']:\n",
    "        if col in merged_data_clean.columns:\n",
    "            merged_data_clean[col] = remaining_imputer.fit_transform(merged_data_clean[[col]])\n",
    "            print(f\"Imputed missing values in '{col}' with mean.\")\n",
    "        else:\n",
    "            print(f\"Warning: '{col}' column not found in merged_data_clean.\")\n",
    "\n",
    "    # Final check for missing values after imputation\n",
    "    print(\"\\nMissing values in merged_data_clean after all imputations:\")\n",
    "    print(merged_data_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 11. Feature Selection and Splitting\n",
    "# ==============================================\n",
    "\n",
    "# Define the target variable\n",
    "target = 'Fraud_Count'\n",
    "\n",
    "# Define features: all columns except 'State', 'Year', and 'Fraud_Count'\n",
    "features = [col for col in merged_data_clean.columns if col not in ['State', 'Year', 'Fraud_Count']]\n",
    "\n",
    "# Display selected features\n",
    "print(\"\\nSelected Features for Analysis:\")\n",
    "print(features)\n",
    "\n",
    "# Extract feature matrix (X) and target vector (y)\n",
    "X = merged_data_clean[features]\n",
    "y = merged_data_clean[target]\n",
    "\n",
    "# Check the number of samples and features\n",
    "print(f\"\\nNumber of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 12. Train the Decision Tree Regressor\n",
    "# ==============================================\n",
    "\n",
    "# Initialize the Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nDecision Tree Regressor has been trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 13. Evaluate the Decision Tree Model\n",
    "# ==============================================\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nDecision Tree Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 14. Determine Feature Importance from Decision Tree\n",
    "# ==============================================\n",
    "\n",
    "# Get feature importances from the model\n",
    "importances = dt_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importances from Decision Tree Regressor', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 15. Train and Evaluate a Random Forest Regressor\n",
    "# ==============================================\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"\\nRandom Forest Regressor has been trained successfully!\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_rf:.2f}\")\n",
    "print(f\"R-squared: {r2_rf:.4f}\")\n",
    "\n",
    "# Extract feature importances from the Random Forest model\n",
    "importances_rf = rf_model.feature_importances_\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances_rf\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importances\n",
    "print(\"\\nRandom Forest Feature Importances:\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_rf, x='Importance', y='Feature', palette='magma')\n",
    "plt.title('Feature Importances from Random Forest Regressor', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 16. Hyperparameter Tuning for Random Forest\n",
    "# ==============================================\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "print(\"\\nStarting Grid Search for Random Forest...\")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"\\nBest parameters found for Random Forest:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict with the best model\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "\n",
    "print(f\"\\nBest Random Forest Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_best_rf:.2f}\")\n",
    "print(f\"R-squared: {r2_best_rf:.4f}\")\n",
    "\n",
    "# Feature Importances from the best Random Forest model\n",
    "importances_best_rf = best_rf_model.feature_importances_\n",
    "feature_importance_best_rf = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances_best_rf\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nBest Random Forest Feature Importances:\")\n",
    "print(feature_importance_best_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_best_rf, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importances from Best Random Forest Regressor', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 17. Cross-Validation for Random Forest\n",
    "# ==============================================\n",
    "\n",
    "# Initialize the model with best parameters\n",
    "rf_model_cv = RandomForestRegressor(\n",
    "    n_estimators=best_rf_model.n_estimators,\n",
    "    max_depth=best_rf_model.max_depth,\n",
    "    min_samples_split=best_rf_model.min_samples_split,\n",
    "    min_samples_leaf=best_rf_model.min_samples_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_model_cv, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"\\nCross-Validation R-squared Scores:\")\n",
    "print(cv_scores)\n",
    "print(f\"Mean R-squared: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 18. Feature Engineering with Polynomial Features (Optional)\n",
    "# ==============================================\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "poly_features = poly.get_feature_names_out(features)\n",
    "X_poly = pd.DataFrame(X_poly, columns=poly_features)\n",
    "\n",
    "# Split the polynomial features\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a new Random Forest model on polynomial features\n",
    "rf_model_poly = RandomForestRegressor(\n",
    "    n_estimators=best_rf_model.n_estimators,\n",
    "    max_depth=best_rf_model.max_depth,\n",
    "    min_samples_split=best_rf_model.min_samples_split,\n",
    "    min_samples_leaf=best_rf_model.min_samples_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model_poly.fit(X_train_poly, y_train_poly)\n",
    "y_pred_poly = rf_model_poly.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the new model\n",
    "mse_poly = mean_squared_error(y_test_poly, y_pred_poly)\n",
    "r2_poly = r2_score(y_test_poly, y_pred_poly)\n",
    "\n",
    "print(\"\\nRandom Forest Regressor with Polynomial Features Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_poly:.2f}\")\n",
    "print(f\"R-squared: {r2_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 19. Final Recommendations and Next Steps\n",
    "# ==============================================\n",
    "\n",
    "# Based on the performance metrics, consider further steps such as:\n",
    "# - Feature Engineering (creating interaction terms, polynomial features)\n",
    "# - Trying different models (e.g., Gradient Boosting, SVR, Neural Networks)\n",
    "# - Handling outliers and skewed target distributions\n",
    "# - Implementing feature selection techniques (e.g., Recursive Feature Elimination)\n",
    "\n",
    "# Example: Training Gradient Boosting Regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"\\nGradient Boosting Regressor Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_gb:.2f}\")\n",
    "print(f\"R-squared: {r2_gb:.4f}\")\n",
    "\n",
    "# Feature Importances from Gradient Boosting\n",
    "importances_gb = gb_model.feature_importances_\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances_gb\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nGradient Boosting Feature Importances:\")\n",
    "print(feature_importance_gb)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_gb, x='Importance', y='Feature', palette='coolwarm')\n",
    "plt.title('Feature Importances from Gradient Boosting Regressor', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 1. Identify Top States with Highest Fraud Counts\n",
    "# ==============================================\n",
    "\n",
    "# Aggregate total Fraud_Count per state\n",
    "total_fraud_by_state = merged_data_clean.groupby('State')['Fraud_Count'].sum().reset_index()\n",
    "\n",
    "# Sort states by total Fraud_Count in descending order\n",
    "total_fraud_by_state = total_fraud_by_state.sort_values(by='Fraud_Count', ascending=False)\n",
    "\n",
    "# Define the number of top states to analyze\n",
    "top_n = 10\n",
    "\n",
    "# Select top N states\n",
    "top_states = total_fraud_by_state.head(top_n)['State'].tolist()\n",
    "\n",
    "print(f\"Top {top_n} States with Highest Total Fraud Counts:\")\n",
    "print(total_fraud_by_state.head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2. Filter Data for Top States\n",
    "# ==============================================\n",
    "\n",
    "# Filter the merged_data_clean for top states\n",
    "top_states_data = merged_data_clean[merged_data_clean['State'].isin(top_states)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFiltered Data for Top {top_n} States:\")\n",
    "print(top_states_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 3. Train Models for Each Top State\n",
    "# ==============================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize a dictionary to store feature importances per state\n",
    "feature_importances_dict = {}\n",
    "\n",
    "# Loop through each top state\n",
    "for state in top_states:\n",
    "    # Filter data for the current state\n",
    "    state_data = top_states_data[top_states_data['State'] == state]\n",
    "    \n",
    "    # Define features and target\n",
    "    X_state = state_data[features]\n",
    "    y_state = state_data['Fraud_Count']\n",
    "    \n",
    "    # Check if there are enough samples to train the model\n",
    "    if len(state_data) < 5:\n",
    "        print(f\"\\nNot enough data to train a model for {state.title()} (only {len(state_data)} records). Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(\n",
    "        X_state, y_state, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize the Random Forest Regressor\n",
    "    rf_model_state = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model_state.fit(X_train_state, y_train_state)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_state = rf_model_state.predict(X_test_state)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse_state = mean_squared_error(y_test_state, y_pred_state)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    print(f\"\\n{state.title()} - Random Forest Regressor Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_state:.2f}\")\n",
    "    print(f\"R-squared: {r2_state:.4f}\")\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances_state = rf_model_state.feature_importances_\n",
    "    feature_importances_state = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importances_state\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    feature_importances_dict[state] = feature_importances_state\n",
    "    \n",
    "    # Display feature importances\n",
    "    print(f\"\\nFeature Importances for {state.title()}:\")\n",
    "    print(feature_importances_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 4. Extract and Visualize Feature Importances\n",
    "# ==============================================\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Iterate through the feature importances dictionary and plot\n",
    "for state, fi in feature_importances_dict.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=fi, palette='viridis')\n",
    "    plt.title(f'Feature Importances in {state.title()}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.xlim(0, fi['Importance'].max() + 0.05)  # Adjust x-axis for better visualization\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 6. Normalize Fraud Counts by Population\n",
    "# ==============================================\n",
    "\n",
    "# Calculate fraud rate per 10,000 population\n",
    "top_states_data['Fraud_Rate'] = (top_states_data['Fraud_Count'] / top_states_data['Total_Population']) * 10000\n",
    "\n",
    "# Display the updated data\n",
    "print(\"\\nTop States Data with Fraud Rate:\")\n",
    "print(top_states_data[['State', 'Year', 'Fraud_Count', 'Total_Population', 'Fraud_Rate']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 7. Correlation Analysis\n",
    "# ==============================================\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = top_states_data[['Fraud_Rate'] + features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 8. Scatter Plots for Feature Relationships\n",
    "# ==============================================\n",
    "\n",
    "# Define features to plot against Fraud_Rate\n",
    "features_to_plot = ['Number_in_Poverty', 'Poverty_Rate', 'Total_Population', 'HPI', 'Unemployment_Rate']\n",
    "\n",
    "for feature in features_to_plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=feature, y='Fraud_Rate', data=top_states_data, hue='State', palette='tab10')\n",
    "    plt.title(f'{feature} vs Fraud Rate')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Fraud Rate (per 10,000 population)')\n",
    "    plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Top 10 States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 1. Load the Cargo Fraud Data\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visual style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load the CSV file\n",
    "cargo_fraud = pd.read_csv('cargo_fraud_only.csv')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"Sample of Cargo Fraud Data:\")\n",
    "print(cargo_fraud.head())\n",
    "\n",
    "# Check the total number of records and columns\n",
    "print(f\"\\nTotal records: {cargo_fraud.shape[0]}\")\n",
    "print(f\"Total columns: {cargo_fraud.shape[1]}\")\n",
    "\n",
    "# Get a summary of the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(cargo_fraud.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(cargo_fraud.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 2. Count Cargo Frauds per State\n",
    "# ==============================================\n",
    "\n",
    "# Aggregate the number of fraud cases per state\n",
    "fraud_counts = cargo_fraud.groupby('state_name').size().reset_index(name='Fraud_Count')\n",
    "\n",
    "# Sort the states by Fraud_Count in descending order\n",
    "fraud_counts_sorted = fraud_counts.sort_values(by='Fraud_Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Select the top 10 states\n",
    "top_10_states = fraud_counts_sorted.head(10)\n",
    "\n",
    "print(f\"\\nTop 10 States with Highest Cargo Fraud Counts:\")\n",
    "print(top_10_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 3. Visualize Top 10 States with Highest Cargo Fraud Counts\n",
    "# ==============================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=top_10_states, x='Fraud_Count', y='state_name', palette='viridis')\n",
    "plt.title('Top 10 States with Highest Cargo Fraud Counts', fontsize=16)\n",
    "plt.xlabel('Total Fraud Counts', fontsize=14)\n",
    "plt.ylabel('State', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
