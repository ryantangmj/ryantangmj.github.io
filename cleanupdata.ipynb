{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Project\n",
    "\n",
    "## Collaboration Plan\n",
    "**Participants:** Ryan Tang and Santiago von Straussburg\n",
    "\n",
    "**Github Page**: https://ryantangmj.github.io\n",
    "\n",
    "### Overview\n",
    "We are collaboratively analyzing two fraud datasets to explore fraud patterns, feature importance, and machine learning model evaluation.\n",
    "\n",
    "- **First dataset:** `Cargo_fraud_only.csv`, obtained from [UCR Database](https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/downloads)\n",
    "- **Second dataset:** Twelve-month and one-month arrest data for fraud, obtained from [UCR Database](https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/downloads)\n",
    "- **Third dataset:** Yearly unemployment index by State, obtained from [BLS Database](https://data.bls.gov/lausmap/showMap.jsp)\n",
    "\n",
    "\n",
    "### Technologies Used\n",
    "1. **GitHub Repository**: For version control, code collaboration, and final project hosting.\n",
    "2. **Google Colab/Jupyter Notebooks**: For ETL, EDA, and model development.\n",
    "3. **Discord**: Primary communication platform for real-time discussions.\n",
    "   - Weekly meetings at 08:00 PM CST on Thursdays for progress reviews and planning.\n",
    "\n",
    "### Current Focus\n",
    "Both team members are currently working together on data exploration, including Extraction, Transformation, and Load (ETL) processes, as well as Exploratory Data Analysis (EDA).\n",
    "\n",
    "### Roadmap & Milestones\n",
    "#### Milestone 1 – Initial Dataset Selection & ETL\n",
    "- Identify datasets. - **Completed**\n",
    "- Perform initial ETL on datasets. - **Completed**\n",
    "- Establish a GitHub repository and GitHub Pages site. - **Completed**\n",
    "- Develop basic statistics and initial graph for dataset understanding. - **Completed**\n",
    "\n",
    "#### Milestone 2 – Additional ETL & Exploratory Data Analysis\n",
    "- Continue data cleaning and transformation. - **Completed**\n",
    "- Conduct comprehensive EDA with 3-5 key graphs. - **Completed**\n",
    "- Present the first project pitch with initial findings. - **Completed**\n",
    "\n",
    "#### Deliverable 1 – In-Class Presentation\n",
    "- Finalize and present a 5-7 slide deck covering problem statement, ETL, EDA, and project progress. - **Completed**\n",
    "\n",
    "#### Milestone 3 – Model Development & Evaluation\n",
    "- Select machine learning models (Random Forest, XGBoost, Logistic Regression).\n",
    "- Begin model training and evaluation.\n",
    "- Analyze model performance and feature importance.\n",
    "\n",
    "#### Deliverable 2 – Final Website & Presentation\n",
    "- Finalize project with the deployment of results to the GitHub Pages site.\n",
    "- Prepare the final presentation summarizing the project lifecycle.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "The goal of this collaborative project is to analyze fraud patterns, identify significant features contributing to fraud, and evaluate various machine learning models for fraud detection. By leveraging two distinct datasets, we aim to develop a deep understanding of fraudulent behavior and build predictive models that will aid in identifying and mitigating fraud across different sectors. Specifically, our objectives are as follows:\n",
    "\n",
    "### 1. Fraud Pattern Analysis\n",
    "- **Objective:** Investigate patterns and trends in fraud activities across different sectors, particularly cargo-related fraud and arrest data for fraud cases. This will involve examining how fraudulent activities vary over time and geographic locations, and identifying key factors that influence fraud prevalence.\n",
    "\n",
    "### 2. Feature Importance Assessment\n",
    "- **Objective:** Analyze and rank the importance of different features contributing to fraud detection. By evaluating features such as transaction details, timestamps, geographic data, and socio-economic indicators, we aim to pinpoint the key variables that can most accurately predict fraud occurrences.\n",
    "\n",
    "### 3. Machine Learning Model Development & Evaluation\n",
    "- **Objective:** Develop and compare multiple machine learning models (Random Forest, XGBoost, Logistic Regression) to identify the most effective model for predicting fraud cases. The models will be evaluated on their performance using metrics such as accuracy, precision, recall, and F1-score to ensure robust fraud detection capabilities.\n",
    "\n",
    "### 4. Comprehensive Data Analysis\n",
    "- **Objective:** Through thorough data exploration and analysis, we aim to create a holistic view of fraud activities, utilizing exploratory data analysis (EDA) techniques. This will include visualizing fraud trends, uncovering hidden relationships, and establishing baseline statistics.\n",
    "\n",
    "### 5. Actionable Insights & Final Presentation\n",
    "- **Objective:** By the end of the project, we aim to deliver a comprehensive set of insights that can inform decision-making regarding fraud prevention and detection strategies. These findings will be shared through a final presentation and a dedicated project website hosted via GitHub Pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from scipy.stats import linregress\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cargo fraud data\n",
    "cargo_fraud = pd.read_csv('cargo_fraud_only.csv')\n",
    "\n",
    "# Check if 'data_year' column exists\n",
    "if 'data_year' in cargo_fraud.columns:\n",
    "    # Use 'data_year' as 'Year'\n",
    "    cargo_fraud['Year'] = cargo_fraud['data_year'].astype(int)\n",
    "else:\n",
    "    # Extract 'Year' from 'date_recovered' or another date column\n",
    "    cargo_fraud['date_recovered'] = pd.to_datetime(cargo_fraud['date_recovered'], errors='coerce')\n",
    "    cargo_fraud['Year'] = cargo_fraud['date_recovered'].dt.year\n",
    "    cargo_fraud = cargo_fraud.dropna(subset=['Year'])\n",
    "    cargo_fraud['Year'] = cargo_fraud['Year'].astype(int)\n",
    "\n",
    "# Ensure the 'State' column exists and matches the 'State' in 'state_unemployment'\n",
    "if 'state_name' in cargo_fraud.columns:\n",
    "    cargo_fraud['State'] = cargo_fraud['state_name']\n",
    "else:\n",
    "    # Map state abbreviations to full state names\n",
    "    state_abbrev_to_name = {\n",
    "        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',\n",
    "        'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',\n",
    "        'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
    "        'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "        'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "        'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "        'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',\n",
    "        'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',\n",
    "        'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',\n",
    "        'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',\n",
    "        'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "    }\n",
    "    cargo_fraud['State'] = cargo_fraud['state_abbr'].map(state_abbrev_to_name)\n",
    "\n",
    "# Exclude rows with missing 'State' or 'Year'\n",
    "cargo_fraud = cargo_fraud.dropna(subset=['State', 'Year'])\n",
    "\n",
    "# Exclude Puerto Rico and other territories\n",
    "cargo_fraud = cargo_fraud[~cargo_fraud['State'].isin(exclude_states)]\n",
    "\n",
    "# Display the processed cargo fraud data\n",
    "print(cargo_fraud[['State', 'Year']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load arrest data\n",
    "arrest_data_url = \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/main/fraud_onemonth_data.csv\"\n",
    "arrest_data = pd.read_csv(arrest_data_url, index_col=0)\n",
    "arrest_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "arrest_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HPI data\n",
    "hpi_data_url = \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/main/hpi_by_state.csv\"\n",
    "hpi_data = pd.read_csv(hpi_data_url)\n",
    "\n",
    "# Keep relevant columns\n",
    "hpi_data = hpi_data[[\"State\", \"Year\", \"HPI\"]]\n",
    "\n",
    "# Filter years between 2012 and 2022\n",
    "hpi_data = hpi_data[(hpi_data[\"Year\"] >= 2012) & (hpi_data[\"Year\"] <= 2022)].reset_index(drop=True)\n",
    "\n",
    "hpi_data['State'] = hpi_data['State'].str.lower()\n",
    "hpi_data['Year'] = hpi_data['Year'].astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "hpi_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     State  Year  Number_in_Poverty  Poverty_Rate  Total_Population\n",
      "0  alabama  2012              777.0          16.2            4808.0\n",
      "1  alabama  2013              891.0          18.5            4807.0\n",
      "2  alabama  2014              848.0          17.8            4765.0\n",
      "3  alabama  2015              784.0          16.3            4820.0\n",
      "4  alabama  2016              782.0          16.2            4821.0\n"
     ]
    }
   ],
   "source": [
    "# Load poverty data\n",
    "poverty_data = pd.read_csv('poverty_data.csv')  # Replace with the actual file path\n",
    "\n",
    "# Reshape poverty data to long format\n",
    "id_vars = ['State']\n",
    "value_vars = [col for col in poverty_data.columns if col != 'State']\n",
    "\n",
    "poverty_long = pd.melt(poverty_data, id_vars=id_vars, value_vars=value_vars,\n",
    "                       var_name='Variable', value_name='Value')\n",
    "\n",
    "poverty_long['Year'] = poverty_long['Variable'].str.extract('(\\d{4})', expand=False)\n",
    "poverty_long['Variable_Name'] = poverty_long['Variable'].str.replace(' \\d{4}', '', regex=True)\n",
    "\n",
    "poverty_pivot = poverty_long.pivot_table(index=['State', 'Year'], columns='Variable_Name', values='Value', aggfunc='first').reset_index()\n",
    "poverty_pivot.columns.name = None\n",
    "\n",
    "poverty_pivot['Year'] = poverty_pivot['Year'].astype(int)\n",
    "poverty_pivot['State'] = poverty_pivot['State'].str.lower()\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = ['Total population', 'Number in poverty', 'Percentage poverty']\n",
    "for col in numeric_cols:\n",
    "    # Convert the column to string first, handling NaN values\n",
    "    poverty_pivot[col] = poverty_pivot[col].astype(str).replace('nan', '').str.replace(',', '')\n",
    "    poverty_pivot[col] = poverty_pivot[col].replace('', pd.NA)\n",
    "    poverty_pivot[col] = poverty_pivot[col].astype(float)\n",
    "\n",
    "# Rename columns for clarity\n",
    "poverty_pivot.rename(columns={\n",
    "    'Total population': 'Total_Population',\n",
    "    'Number in poverty': 'Number_in_Poverty',\n",
    "    'Percentage poverty': 'Poverty_Rate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(poverty_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load homelessness data\n",
    "homelessness_data_url = \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/main/homeless_data.csv\"\n",
    "homelessness_data = pd.read_csv(homelessness_data_url)\n",
    "\n",
    "# Display the first few rows\n",
    "homelessness_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State  Year  2003 Urban Influence Code  2013 Rural-urban Continuum Code  \\\n",
      "0    ak  1970                        NaN                              NaN   \n",
      "1    ak  1980                        NaN                              NaN   \n",
      "2    ak  1990                        NaN                              NaN   \n",
      "3    ak  2000                        NaN                              NaN   \n",
      "4    ak  2003                       12.0                              NaN   \n",
      "\n",
      "   2013 Urban Influence Code  2023 Rural-urban Continuum Code  \\\n",
      "0                        NaN                              NaN   \n",
      "1                        NaN                              NaN   \n",
      "2                        NaN                              NaN   \n",
      "3                        NaN                              NaN   \n",
      "4                        NaN                              NaN   \n",
      "\n",
      "   Bachelor's degree or higher,  Bachelor's degree or higher,-12  \\\n",
      "0                           NaN                              NaN   \n",
      "1                           NaN                              NaN   \n",
      "2                       74497.0                              NaN   \n",
      "3                       93807.0                              NaN   \n",
      "4                           NaN                              NaN   \n",
      "\n",
      "   Bachelor's degree or higher,-22  Four years of college or higher,  ...  \\\n",
      "0                              NaN                           19082.0  ...   \n",
      "1                              NaN                           44554.0  ...   \n",
      "2                              NaN                               NaN  ...   \n",
      "3                              NaN                               NaN  ...   \n",
      "4                              NaN                               NaN  ...   \n",
      "\n",
      "   Percent of adults with a high school diploma only,  \\\n",
      "0                                               37.7    \n",
      "1                                               38.9    \n",
      "2                                               28.7    \n",
      "3                                               27.9    \n",
      "4                                                NaN    \n",
      "\n",
      "   Percent of adults with a high school diploma only,-12  \\\n",
      "0                                                NaN       \n",
      "1                                                NaN       \n",
      "2                                                NaN       \n",
      "3                                                NaN       \n",
      "4                                                NaN       \n",
      "\n",
      "   Percent of adults with a high school diploma only,-22  \\\n",
      "0                                                NaN       \n",
      "1                                                NaN       \n",
      "2                                                NaN       \n",
      "3                                                NaN       \n",
      "4                                                NaN       \n",
      "\n",
      "   Percent of adults with less than a high school diploma,  \\\n",
      "0                                               33.3         \n",
      "1                                               17.5         \n",
      "2                                               13.4         \n",
      "3                                               11.7         \n",
      "4                                                NaN         \n",
      "\n",
      "   Percent of adults with less than a high school diploma,-12  \\\n",
      "0                                                NaN            \n",
      "1                                                NaN            \n",
      "2                                                NaN            \n",
      "3                                                NaN            \n",
      "4                                                NaN            \n",
      "\n",
      "   Percent of adults with less than a high school diploma,-22  \\\n",
      "0                                                NaN            \n",
      "1                                                NaN            \n",
      "2                                                NaN            \n",
      "3                                                NaN            \n",
      "4                                                NaN            \n",
      "\n",
      "   Some college (1-3 years),  Some college or associate's degree,  \\\n",
      "0                    20052.0                                  NaN   \n",
      "1                    47722.0                                  NaN   \n",
      "2                        NaN                             112763.0   \n",
      "3                        NaN                             135655.0   \n",
      "4                        NaN                                  NaN   \n",
      "\n",
      "   Some college or associate's degree,-12  \\\n",
      "0                                     NaN   \n",
      "1                                     NaN   \n",
      "2                                     NaN   \n",
      "3                                     NaN   \n",
      "4                                     NaN   \n",
      "\n",
      "   Some college or associate's degree,-22  \n",
      "0                                     NaN  \n",
      "1                                     NaN  \n",
      "2                                     NaN  \n",
      "3                                     NaN  \n",
      "4                                     NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Process Education Data ---\n",
    "\n",
    "# Load education data\n",
    "education_data = pd.read_csv('education.csv')  # Replace with the actual file path\n",
    "\n",
    "# Reshape education data to long format\n",
    "id_vars = ['State']\n",
    "value_vars = [col for col in education_data.columns if col != 'State']\n",
    "\n",
    "education_long = pd.melt(\n",
    "    education_data,\n",
    "    id_vars=id_vars,\n",
    "    value_vars=value_vars,\n",
    "    var_name='Variable',\n",
    "    value_name='Value'\n",
    ")\n",
    "\n",
    "# Extract 'Year' and 'Variable_Name' from the 'Variable' column\n",
    "education_long['Year'] = education_long['Variable'].str.extract('(\\d{4})', expand=False)\n",
    "education_long['Variable_Name'] = education_long['Variable'].str.replace(' \\d{4}', '', regex=True)\n",
    "\n",
    "# Pivot the data to have one row per 'State' and 'Year'\n",
    "education_pivot = education_long.pivot_table(\n",
    "    index=['State', 'Year'],\n",
    "    columns='Variable_Name',\n",
    "    values='Value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the columns\n",
    "education_pivot.columns.name = None\n",
    "\n",
    "# Convert data types\n",
    "education_pivot['Year'] = education_pivot['Year'].astype(int)\n",
    "education_pivot['State'] = education_pivot['State'].str.lower()\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = [col for col in education_pivot.columns if col not in ['State', 'Year']]\n",
    "for col in numeric_cols:\n",
    "    # Convert to string, remove commas\n",
    "    education_pivot[col] = education_pivot[col].astype(str).str.replace(',', '')\n",
    "    # Convert to numeric, coercing errors to NaN\n",
    "    education_pivot[col] = pd.to_numeric(education_pivot[col], errors='coerce')\n",
    "\n",
    "# Rename columns as needed\n",
    "# For example, if you have 'Percentage with Bachelor's Degree'\n",
    "# education_pivot.rename(columns={\n",
    "#     \"Percentage with Bachelor's Degree\": 'Bachelor_Degree_Rate'\n",
    "# }, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(education_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index and check for missing values\n",
    "cargo_fraud.reset_index(drop=True, inplace=True)\n",
    "print(cargo_fraud.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'State Code' to string and strip whitespace\n",
    "arrest_data['State Code'] = arrest_data['State Code'].astype(str).str.strip()\n",
    "\n",
    "# Map state codes to state names\n",
    "state_codes = {\n",
    "    '50': 'Alaska',\n",
    "    '01': 'Alabama',\n",
    "    '03': 'Arkansas',\n",
    "    '54': 'American Samoa',\n",
    "    '02': 'Arizona',\n",
    "    '04': 'California',\n",
    "    '05': 'Colorado',\n",
    "    '06': 'Connecticut',\n",
    "    '52': 'Canal Zone',\n",
    "    '08': 'District of Columbia',\n",
    "    '07': 'Delaware',\n",
    "    '09': 'Florida',\n",
    "    '10': 'Georgia',\n",
    "    '55': 'Guam',\n",
    "    '51': 'Hawaii',\n",
    "    '14': 'Iowa',\n",
    "    '11': 'Idaho',\n",
    "    '12': 'Illinois',\n",
    "    '13': 'Indiana',\n",
    "    '15': 'Kansas',\n",
    "    '16': 'Kentucky',\n",
    "    '17': 'Louisiana',\n",
    "    '20': 'Massachusetts',\n",
    "    '19': 'Maryland',\n",
    "    '18': 'Maine',\n",
    "    '21': 'Michigan',\n",
    "    '22': 'Minnesota',\n",
    "    '24': 'Missouri',\n",
    "    '23': 'Mississippi',\n",
    "    '25': 'Montana',\n",
    "    '26': 'Nebraska',\n",
    "    '32': 'North Carolina',\n",
    "    '33': 'North Dakota',\n",
    "    '28': 'New Hampshire',\n",
    "    '29': 'New Jersey',\n",
    "    '30': 'New Mexico',\n",
    "    '27': 'Nevada',\n",
    "    '31': 'New York',\n",
    "    '34': 'Ohio',\n",
    "    '35': 'Oklahoma',\n",
    "    '36': 'Oregon',\n",
    "    '37': 'Pennsylvania',\n",
    "    '53': 'Puerto Rico',\n",
    "    '38': 'Rhode Island',\n",
    "    '39': 'South Carolina',\n",
    "    '40': 'South Dakota',\n",
    "    '41': 'Tennessee',\n",
    "    '42': 'Texas',\n",
    "    '43': 'Utah',\n",
    "    '62': 'Virgin Islands',\n",
    "    '45': 'Virginia',\n",
    "    '44': 'Vermont',\n",
    "    '46': 'Washington',\n",
    "    '48': 'Wisconsin',\n",
    "    '47': 'West Virginia',\n",
    "    '49': 'Wyoming'\n",
    "}\n",
    "\n",
    "arrest_data['State'] = arrest_data['State Code'].map(state_codes)\n",
    "\n",
    "# Display the first few rows\n",
    "arrest_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unemployment data\n",
    "unemployment_data = pd.read_csv('Unemployment.csv')\n",
    "\n",
    "# Identify columns that contain 'Unemployment_rate'\n",
    "unemployment_rate_cols = [col for col in unemployment_data.columns if 'Unemployment_rate_' in col]\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "unemployment_long = pd.melt(\n",
    "    unemployment_data,\n",
    "    id_vars=['State', 'Area_Name'],\n",
    "    value_vars=unemployment_rate_cols,\n",
    "    var_name='Year',\n",
    "    value_name='Unemployment_Rate'\n",
    ")\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "unemployment_long['Year'] = unemployment_long['Year'].str.extract('Unemployment_rate_(\\d+)', expand=False)\n",
    "unemployment_long['Year'] = unemployment_long['Year'].astype(int)\n",
    "\n",
    "# Convert 'Unemployment_Rate' to numeric\n",
    "unemployment_long['Unemployment_Rate'] = pd.to_numeric(unemployment_long['Unemployment_Rate'], errors='coerce')\n",
    "\n",
    "# Aggregate at the state level by taking the mean unemployment rate for each state and year\n",
    "state_unemployment = unemployment_long.groupby(['State', 'Year'])['Unemployment_Rate'].mean().reset_index()\n",
    "\n",
    "# Exclude territories and focus on continental US\n",
    "exclude_states = ['Puerto Rico', 'Guam', 'Virgin Islands', 'American Samoa', 'Northern Mariana Islands', 'Alaska', 'Hawaii']\n",
    "state_unemployment = state_unemployment[~state_unemployment['State'].isin(exclude_states)]\n",
    "\n",
    "# Display the processed unemployment data\n",
    "print(state_unemployment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map state abbreviations to full state names\n",
    "state_abbreviations = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NY': 'New York',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming',\n",
    "    'DC': 'District of Columbia',\n",
    "    'AS': 'American Samoa',\n",
    "    'GU': 'Guam',\n",
    "    'MP': 'Northern Mariana Islands',\n",
    "    'PR': 'Puerto Rico',\n",
    "    'VI': 'Virgin Islands'\n",
    "}\n",
    "\n",
    "homelessness_data['State'] = homelessness_data['State'].map(state_abbreviations)\n",
    "\n",
    "# Rename columns for years\n",
    "new_column_names = {\n",
    "    col: col.split(', ')[-1].split('-')[0] for col in homelessness_data.columns if 'Change in Total Homelessness' in col\n",
    "}\n",
    "homelessness_data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df_homelessness = pd.melt(homelessness_data, id_vars=['State'], var_name='Year', value_name='homeless_rate_change')\n",
    "\n",
    "# Clean 'homeless_rate_change' column\n",
    "df_homelessness['homeless_rate_change'].replace(' ', np.nan, inplace=True)\n",
    "df_homelessness.dropna(inplace=True)\n",
    "df_homelessness['homeless_rate_change'] = df_homelessness['homeless_rate_change'].str.replace('%', '').astype(float)\n",
    "df_homelessness['Year'] = df_homelessness['Year'].astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "df_homelessness.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns that start with 'Percentage poverty' and include 'State'\n",
    "filtered_poverty_data = poverty_data[[col for col in poverty_data.columns if col.startswith('Percentage poverty') or col == 'State']]\n",
    "\n",
    "# Rename columns\n",
    "new_column_names = {\n",
    "    col: col.split(' ')[-1] for col in filtered_poverty_data.columns\n",
    "}\n",
    "filtered_poverty_data.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df_poverty = pd.melt(filtered_poverty_data, id_vars=['State'], var_name='Year', value_name='Percentage Poverty')\n",
    "df_poverty['Year'] = df_poverty['Year'].astype(int)\n",
    "\n",
    "# Display the first few rows\n",
    "df_poverty.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of fraud by region\n",
    "fraud_by_region = cargo_fraud['region_name'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# Plot the proportion of fraud occurrences by region\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=fraud_by_region.values, y=fraud_by_region.index, palette=\"coolwarm\", edgecolor=\"black\")\n",
    "plt.title(\"Proportion of Fraud Occurrences by Region\", fontsize=20, weight=\"bold\")\n",
    "plt.xlabel(\"Proportion of Fraud Occurrences\", fontsize=14, weight=\"bold\")\n",
    "plt.ylabel(\"Region\", fontsize=14, weight=\"bold\")\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of fraud by state\n",
    "fraud_by_state = cargo_fraud['state_name'].value_counts(normalize=True).sort_values(ascending=False).reset_index()\n",
    "fraud_by_state.columns = ['NAME', 'proportion']\n",
    "\n",
    "# Load USA shapefile\n",
    "usa = gpd.read_file('https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip')\n",
    "\n",
    "# Merge the geopandas file and fraud data\n",
    "merged = usa.merge(fraud_by_state, on='NAME', how='left')\n",
    "merged['proportion'] = merged['proportion'].fillna(0)\n",
    "\n",
    "# Exclude Alaska and Hawaii before plotting\n",
    "merged = merged[~merged['STUSPS'].isin(['AK', 'HI'])]\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(18, 12))\n",
    "merged.plot(\n",
    "    column='proportion', cmap='coolwarm', linewidth=0.6,\n",
    "    ax=ax, edgecolor='black', legend=True,\n",
    "    legend_kwds={\n",
    "        'label': \"Proportion of Fraud Occurrences by State\",\n",
    "        'orientation': \"horizontal\",\n",
    "        'shrink': 0.8\n",
    "    }\n",
    ")\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Proportion of Fraud Occurrences by State', fontsize=20, weight=\"bold\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of fraud by race\n",
    "fraud_by_race = cargo_fraud['offender_race'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# Plot the proportion of fraud occurrences by race\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=fraud_by_race.values, y=fraud_by_race.index, palette=\"viridis\", edgecolor=\"black\")\n",
    "plt.title(\"Proportion of Fraud Occurrences by Race\", fontsize=20, weight=\"bold\")\n",
    "plt.xlabel(\"Proportion of Fraud Occurrences\", fontsize=14, weight=\"bold\")\n",
    "plt.ylabel(\"Race\", fontsize=14, weight=\"bold\")\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts by 'Year'\n",
    "fraud_by_year = cargo_fraud['Year'].value_counts().sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(x=fraud_by_year.index, y=fraud_by_year.values, marker=\"o\", color=\"#5A9BD5\", linewidth=2.5)\n",
    "plt.title(\"Fraudulent Transactions by Year\", fontsize=20, weight=\"bold\")\n",
    "plt.xlabel(\"Year\", fontsize=16, weight=\"bold\")\n",
    "plt.ylabel(\"Number of Fraudulent Transactions\", fontsize=16, weight=\"bold\")\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the unemployment data with cargo fraud data\n",
    "merged_df = pd.merge(cargo_fraud, state_unemployment, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load and Process cargo_fraud Data\n",
    "\n",
    "# Load the cargo fraud data\n",
    "cargo_fraud = pd.read_csv('cargo_fraud_only.csv')\n",
    "\n",
    "# Process cargo_fraud data to extract 'Year' and 'State'\n",
    "\n",
    "# Check if 'data_year' column exists\n",
    "if 'data_year' in cargo_fraud.columns:\n",
    "    cargo_fraud['Year'] = cargo_fraud['data_year'].astype(int)\n",
    "elif 'date_recovered' in cargo_fraud.columns:\n",
    "    cargo_fraud['date_recovered'] = pd.to_datetime(cargo_fraud['date_recovered'], errors='coerce')\n",
    "    cargo_fraud['Year'] = cargo_fraud['date_recovered'].dt.year\n",
    "else:\n",
    "    print(\"No date column found in cargo_fraud data.\")\n",
    "    # Handle the error accordingly\n",
    "    cargo_fraud['Year'] = pd.NA  # Set 'Year' to missing\n",
    "\n",
    "# Ensure 'State' column exists\n",
    "if 'state_name' in cargo_fraud.columns:\n",
    "    cargo_fraud['State'] = cargo_fraud['state_name'].str.lower()\n",
    "elif 'state_abbr' in cargo_fraud.columns:\n",
    "    # Map state abbreviations to full state names\n",
    "    state_abbrev_to_name = {\n",
    "        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas',\n",
    "        'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware',\n",
    "        'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
    "        'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
    "        'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "        'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "        'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada',\n",
    "        'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York',\n",
    "        'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah',\n",
    "        'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia',\n",
    "        'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia'\n",
    "    }\n",
    "    cargo_fraud['State'] = cargo_fraud['state_abbr'].map(state_abbrev_to_name).str.lower()\n",
    "else:\n",
    "    print(\"No state column found in cargo_fraud data.\")\n",
    "    # Handle the error accordingly\n",
    "    cargo_fraud['State'] = pd.NA  # Set 'State' to missing\n",
    "\n",
    "# Exclude any rows with missing 'State' or 'Year'\n",
    "cargo_fraud = cargo_fraud.dropna(subset=['State', 'Year'])\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "cargo_fraud['Year'] = cargo_fraud['Year'].astype(int)\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "exclude_states = ['puerto rico', 'guam', 'virgin islands', 'american samoa',\n",
    "                  'northern mariana islands', 'alaska', 'hawaii']\n",
    "cargo_fraud = cargo_fraud[~cargo_fraud['State'].isin(exclude_states)]\n",
    "\n",
    "# Step 2: Aggregate the number of fraud cases per state and year\n",
    "fraud_counts = cargo_fraud.groupby(['State', 'Year']).size().reset_index(name='Fraud_Count')\n",
    "\n",
    "# Step 3: Load and Process Unemployment Data\n",
    "\n",
    "# Load the unemployment data\n",
    "unemployment_data = pd.read_csv('Unemployment.csv')\n",
    "\n",
    "# Extract state FIPS codes from 'FIPS_Code'\n",
    "unemployment_data['State_FIPS'] = unemployment_data['FIPS_Code'] // 1000\n",
    "\n",
    "# Map state FIPS codes to state names\n",
    "state_fips_to_name = {\n",
    "    1: 'Alabama', 2: 'Alaska', 4: 'Arizona', 5: 'Arkansas',\n",
    "    6: 'California', 8: 'Colorado', 9: 'Connecticut', 10: 'Delaware',\n",
    "    11: 'District of Columbia', 12: 'Florida', 13: 'Georgia', 15: 'Hawaii',\n",
    "    16: 'Idaho', 17: 'Illinois', 18: 'Indiana', 19: 'Iowa',\n",
    "    20: 'Kansas', 21: 'Kentucky', 22: 'Louisiana', 23: 'Maine',\n",
    "    24: 'Maryland', 25: 'Massachusetts', 26: 'Michigan', 27: 'Minnesota',\n",
    "    28: 'Mississippi', 29: 'Missouri', 30: 'Montana', 31: 'Nebraska',\n",
    "    32: 'Nevada', 33: 'New Hampshire', 34: 'New Jersey', 35: 'New Mexico',\n",
    "    36: 'New York', 37: 'North Carolina', 38: 'North Dakota', 39: 'Ohio',\n",
    "    40: 'Oklahoma', 41: 'Oregon', 42: 'Pennsylvania', 44: 'Rhode Island',\n",
    "    45: 'South Carolina', 46: 'South Dakota', 47: 'Tennessee', 48: 'Texas',\n",
    "    49: 'Utah', 50: 'Vermont', 51: 'Virginia', 53: 'Washington',\n",
    "    54: 'West Virginia', 55: 'Wisconsin', 56: 'Wyoming'\n",
    "}\n",
    "unemployment_data['State_Name'] = unemployment_data['State_FIPS'].map(state_fips_to_name)\n",
    "\n",
    "# Exclude rows with missing 'State_Name'\n",
    "missing_states = unemployment_data[unemployment_data['State_Name'].isnull()]\n",
    "if not missing_states.empty:\n",
    "    print(\"Missing state names for the following State_FIPS codes:\")\n",
    "    print(missing_states['State_FIPS'].unique())\n",
    "    # Exclude rows with missing 'State_Name'\n",
    "    unemployment_data = unemployment_data[~unemployment_data['State_Name'].isnull()]\n",
    "\n",
    "# Identify columns that contain 'Unemployment_rate_'\n",
    "unemployment_rate_cols = [col for col in unemployment_data.columns if 'Unemployment_rate_' in col]\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "unemployment_long = pd.melt(\n",
    "    unemployment_data,\n",
    "    id_vars=['State_Name'],\n",
    "    value_vars=unemployment_rate_cols,\n",
    "    var_name='Year',\n",
    "    value_name='Unemployment_Rate'\n",
    ")\n",
    "\n",
    "# Extract the year from the 'Year' column\n",
    "unemployment_long['Year'] = unemployment_long['Year'].str.extract('Unemployment_rate_(\\d+)', expand=False)\n",
    "unemployment_long['Year'] = pd.to_numeric(unemployment_long['Year'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing 'Year'\n",
    "unemployment_long = unemployment_long.dropna(subset=['Year'])\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "unemployment_long['Year'] = unemployment_long['Year'].astype(int)\n",
    "\n",
    "# Convert 'Unemployment_Rate' to numeric\n",
    "unemployment_long['Unemployment_Rate'] = pd.to_numeric(unemployment_long['Unemployment_Rate'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing 'Unemployment_Rate'\n",
    "unemployment_long = unemployment_long.dropna(subset=['Unemployment_Rate'])\n",
    "\n",
    "# Aggregate at the state level by taking the mean unemployment rate for each state and year\n",
    "state_unemployment = unemployment_long.groupby(['State_Name', 'Year'])['Unemployment_Rate'].mean().reset_index()\n",
    "\n",
    "# Rename 'State_Name' to 'State' and convert to lowercase\n",
    "state_unemployment.rename(columns={'State_Name': 'State'}, inplace=True)\n",
    "state_unemployment['State'] = state_unemployment['State'].str.lower()\n",
    "\n",
    "# Exclude territories and non-continental states\n",
    "exclude_states = ['puerto rico', 'guam', 'virgin islands', 'american samoa',\n",
    "                  'northern mariana islands', 'alaska', 'hawaii']\n",
    "state_unemployment = state_unemployment[~state_unemployment['State'].isin(exclude_states)]\n",
    "\n",
    "# Verify exclusion\n",
    "print(\"\\nUnique States in state_unemployment after exclusion:\")\n",
    "print(sorted(state_unemployment['State'].unique()))\n",
    "\n",
    "# Step 4: Prepare 'fraud_counts' DataFrame\n",
    "\n",
    "# Ensure 'State' in fraud_counts is lowercase\n",
    "fraud_counts['State'] = fraud_counts['State'].str.lower()\n",
    "\n",
    "# Exclude 'Federal' and any territories if present\n",
    "fraud_counts = fraud_counts[~fraud_counts['State'].isin(['federal'] + [state.lower() for state in exclude_states])]\n",
    "\n",
    "# Step 5: Merge DataFrames\n",
    "analysis_df = pd.merge(fraud_counts, state_unemployment, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Check for missing values after merging\n",
    "print(\"\\nMissing values in analysis_df after merging:\")\n",
    "print(analysis_df.isnull().sum())\n",
    "\n",
    "# Step 6: Proceed to Plot\n",
    "analysis_df_clean = analysis_df.dropna(subset=['Unemployment_Rate', 'Fraud_Count'])\n",
    "\n",
    "if analysis_df_clean.empty:\n",
    "    print(\"\\nNo data available for plotting after cleaning.\")\n",
    "else:\n",
    "    print(\"\\nData available for plotting.\")\n",
    "    # Plot the relationship\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=analysis_df_clean, x='Unemployment_Rate', y='Fraud_Count')\n",
    "    plt.title('Fraud Count vs Unemployment Rate', fontsize=16)\n",
    "    plt.xlabel('Unemployment Rate (%)', fontsize=14)\n",
    "    plt.ylabel('Fraud Count', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, add a regression line\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.regplot(data=analysis_df_clean, x='Unemployment_Rate', y='Fraud_Count', scatter_kws={'s':50}, line_kws={'color':'red'})\n",
    "    plt.title('Fraud Count vs Unemployment Rate with Regression Line', fontsize=16)\n",
    "    plt.xlabel('Unemployment Rate (%)', fontsize=14)\n",
    "    plt.ylabel('Fraud Count', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and print the correlation coefficient\n",
    "    correlation = analysis_df_clean['Unemployment_Rate'].corr(analysis_df_clean['Fraud_Count'])\n",
    "    print(f\"\\nCorrelation between Unemployment Rate and Fraud Count: {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation and P values\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate correlation coefficient and p-value\n",
    "corr_coef, p_value = pearsonr(analysis_df_clean['Unemployment_Rate'], analysis_df_clean['Fraud_Count'])\n",
    "print(f\"Correlation Coefficient: {corr_coef:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'cargo_fraud' DataFrame is already loaded and processed as before\n",
    "\n",
    "# Aggregate the number of fraud cases per state and year\n",
    "fraud_counts = cargo_fraud.groupby(['State', 'Year']).size().reset_index(name='Fraud_Count')\n",
    "\n",
    "# Ensure 'State' and 'Year' are of appropriate types\n",
    "fraud_counts['State'] = fraud_counts['State'].str.lower()\n",
    "fraud_counts['Year'] = fraud_counts['Year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'State' and 'Year' columns are in the correct format\n",
    "def prepare_dataframe(df, state_col='State', year_col='Year'):\n",
    "    df[state_col] = df[state_col].str.lower()\n",
    "    df[year_col] = df[year_col].astype(int)\n",
    "    return df\n",
    "\n",
    "# Prepare fraud_counts\n",
    "fraud_counts = prepare_dataframe(fraud_counts)\n",
    "\n",
    "# Prepare poverty_pivot\n",
    "poverty_pivot = prepare_dataframe(poverty_pivot)\n",
    "\n",
    "# Prepare hpi_data\n",
    "hpi_data = prepare_dataframe(hpi_data)\n",
    "\n",
    "# Prepare state_unemployment\n",
    "state_unemployment = prepare_dataframe(state_unemployment)\n",
    "\n",
    "# Prepare education_pivot (already prepared in previous steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fraud_counts with poverty_pivot\n",
    "merged_data = pd.merge(fraud_counts, poverty_pivot, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Merge with education_pivot\n",
    "merged_data = pd.merge(merged_data, education_pivot, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Merge with hpi_data\n",
    "merged_data = pd.merge(merged_data, hpi_data, on=['State', 'Year'], how='left')\n",
    "\n",
    "# Merge with state_unemployment\n",
    "merged_data = pd.merge(merged_data, state_unemployment, on=['State', 'Year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in merged_data:\n",
      "State                                                                    0\n",
      "Year                                                                     0\n",
      "Fraud_Count                                                              0\n",
      "Number_in_Poverty                                                        5\n",
      "Poverty_Rate                                                             5\n",
      "Total_Population                                                         5\n",
      "2003 Urban Influence Code                                              338\n",
      "2013 Rural-urban Continuum Code                                        338\n",
      "2013 Urban Influence Code                                              338\n",
      "2023 Rural-urban Continuum Code                                        338\n",
      "Bachelor's degree or higher,                                           338\n",
      "Bachelor's degree or higher,-12                                        338\n",
      "Bachelor's degree or higher,-22                                        338\n",
      "Four years of college or higher,                                       338\n",
      "High school diploma only,                                              338\n",
      "High school diploma only,-12                                           338\n",
      "High school diploma only,-22                                           338\n",
      "Less than a high school diploma,                                       338\n",
      "Less than a high school diploma,-12                                    338\n",
      "Less than a high school diploma,-22                                    338\n",
      "Percent of adults completing four years of college or higher,          338\n",
      "Percent of adults completing some college (1-3 years),                 338\n",
      "Percent of adults completing some college or associate's degree,       338\n",
      "Percent of adults completing some college or associate's degree,-12    338\n",
      "Percent of adults completing some college or associate's degree,-22    338\n",
      "Percent of adults with a bachelor's degree or higher,                  338\n",
      "Percent of adults with a bachelor's degree or higher,-12               338\n",
      "Percent of adults with a bachelor's degree or higher,-22               338\n",
      "Percent of adults with a high school diploma only,                     338\n",
      "Percent of adults with a high school diploma only,-12                  338\n",
      "Percent of adults with a high school diploma only,-22                  338\n",
      "Percent of adults with less than a high school diploma,                338\n",
      "Percent of adults with less than a high school diploma,-12             338\n",
      "Percent of adults with less than a high school diploma,-22             338\n",
      "Some college (1-3 years),                                              338\n",
      "Some college or associate's degree,                                    338\n",
      "Some college or associate's degree,-12                                 338\n",
      "Some college or associate's degree,-22                                 338\n",
      "HPI                                                                      5\n",
      "Unemployment_Rate                                                        5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in merged_data:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Define critical columns for analysis\n",
    "critical_columns = [\n",
    "    'Fraud_Count', 'Poverty_Rate', 'HPI', 'Unemployment_Rate',\n",
    "    # Add your education variables here\n",
    "    # For example:\n",
    "    # 'Bachelor_Degree_Rate',\n",
    "    # 'High_School_Only_Percent',\n",
    "    # 'Less_High_School_Percent',\n",
    "    # 'Some_College_Associate_Percent'\n",
    "]\n",
    "\n",
    "# Drop rows with missing values in critical columns\n",
    "merged_data_clean = merged_data.dropna(subset=critical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing values: 338\n",
      "Number of rows after dropping missing values: 0\n",
      "\n",
      "Selected Features:\n",
      "['Number_in_Poverty', 'Poverty_Rate', 'Total_Population', '2003 Urban Influence Code', '2013 Rural-urban Continuum Code', '2013 Urban Influence Code', '2023 Rural-urban Continuum Code', \"Bachelor's degree or higher,\", \"Bachelor's degree or higher,-12\", \"Bachelor's degree or higher,-22\", 'Four years of college or higher,', 'High school diploma only,', 'High school diploma only,-12', 'High school diploma only,-22', 'Less than a high school diploma,', 'Less than a high school diploma,-12', 'Less than a high school diploma,-22', 'Percent of adults completing four years of college or higher,', 'Percent of adults completing some college (1-3 years),', \"Percent of adults completing some college or associate's degree,\", \"Percent of adults completing some college or associate's degree,-12\", \"Percent of adults completing some college or associate's degree,-22\", \"Percent of adults with a bachelor's degree or higher,\", \"Percent of adults with a bachelor's degree or higher,-12\", \"Percent of adults with a bachelor's degree or higher,-22\", 'Percent of adults with a high school diploma only,', 'Percent of adults with a high school diploma only,-12', 'Percent of adults with a high school diploma only,-22', 'Percent of adults with less than a high school diploma,', 'Percent of adults with less than a high school diploma,-12', 'Percent of adults with less than a high school diploma,-22', 'Some college (1-3 years),', \"Some college or associate's degree,\", \"Some college or associate's degree,-12\", \"Some college or associate's degree,-22\", 'HPI', 'Unemployment_Rate']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m merged_data_clean[target]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets (80% train, 20% test)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 4. Train the CART Model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize the Decision Tree Regressor\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2305\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2302\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2309\u001b[0m     )\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Assuming 'merged_data' is your merged DataFrame from previous steps\n",
    "\n",
    "# 1. Handle Missing Values\n",
    "# Drop rows with any missing values to ensure complete data for modeling\n",
    "merged_data_clean = merged_data.dropna()\n",
    "\n",
    "# Verify the number of rows after dropping\n",
    "print(f\"Number of rows before dropping missing values: {merged_data.shape[0]}\")\n",
    "print(f\"Number of rows after dropping missing values: {merged_data_clean.shape[0]}\")\n",
    "\n",
    "# 2. Select All Features\n",
    "# Define the target variable\n",
    "target = 'Fraud_Count'\n",
    "\n",
    "# Define features: all columns except 'State', 'Year', and 'Fraud_Count'\n",
    "features = [col for col in merged_data_clean.columns if col not in ['State', 'Year', 'Fraud_Count']]\n",
    "\n",
    "# Display selected features\n",
    "print(\"\\nSelected Features:\")\n",
    "print(features)\n",
    "\n",
    "# 3. Prepare Data for Modeling\n",
    "# Extract feature matrix (X) and target vector (y)\n",
    "X = merged_data_clean[features]\n",
    "y = merged_data_clean[target]\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train the CART Model\n",
    "# Initialize the Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nDecision Tree Regressor Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# 6. Determine Feature Importance\n",
    "# Get feature importances from the model\n",
    "importances = dt_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importances from Decision Tree Regressor', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
