{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Project\n",
    "\n",
    "## Collaboration Plan\n",
    "**Participants:** Ryan Tang and Santiago von Straussburg\n",
    "\n",
    "**Github Page**: https://ryantangmj.github.io\n",
    "\n",
    "### Overview\n",
    "We are collaboratively analyzing two fraud datasets to explore fraud patterns, feature importance, and machine learning model evaluation.\n",
    "\n",
    "- **First dataset:** `Cargo_fraud_only.csv`, obtained from [UCR Database](https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/downloads)\n",
    "- **Second dataset:** Twelve-month and one-month arrest data for fraud, obtained from [UCR Database](https://cde.ucr.cjis.gov/LATEST/webapp/#/pages/downloads)\n",
    "- **Third dataset:** Yearly unemployment index by State, obtained from [BLS Database](https://data.bls.gov/lausmap/showMap.jsp)\n",
    "\n",
    "\n",
    "### Technologies Used\n",
    "1. **GitHub Repository**: For version control, code collaboration, and final project hosting.\n",
    "2. **Google Colab/Jupyter Notebooks**: For ETL, EDA, and model development.\n",
    "3. **Discord**: Primary communication platform for real-time discussions.\n",
    "   - Weekly meetings at 08:00 PM CST on Thursdays for progress reviews and planning.\n",
    "\n",
    "### Current Focus\n",
    "Both team members are currently working together on data exploration, including Extraction, Transformation, and Load (ETL) processes, as well as Exploratory Data Analysis (EDA).\n",
    "\n",
    "### Roadmap & Milestones\n",
    "#### Milestone 1 – Initial Dataset Selection & ETL\n",
    "- Identify datasets. - **Completed**\n",
    "- Perform initial ETL on datasets. - **Completed**\n",
    "- Establish a GitHub repository and GitHub Pages site. - **Completed**\n",
    "- Develop basic statistics and initial graph for dataset understanding. - **Completed**\n",
    "\n",
    "#### Milestone 2 – Additional ETL & Exploratory Data Analysis\n",
    "- Continue data cleaning and transformation. - **Completed**\n",
    "- Conduct comprehensive EDA with 3-5 key graphs. - **Completed**\n",
    "- Present the first project pitch with initial findings. - **Completed**\n",
    "\n",
    "#### Deliverable 1 – In-Class Presentation\n",
    "- Finalize and present a 5-7 slide deck covering problem statement, ETL, EDA, and project progress. - **Completed**\n",
    "\n",
    "#### Milestone 3 – Model Development & Evaluation\n",
    "- Select machine learning models (Random Forest, XGBoost, Logistic Regression).\n",
    "- Begin model training and evaluation.\n",
    "- Analyze model performance and feature importance.\n",
    "\n",
    "#### Deliverable 2 – Final Website & Presentation\n",
    "- Finalize project with the deployment of results to the GitHub Pages site.\n",
    "- Prepare the final presentation summarizing the project lifecycle.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "The goal of this collaborative project is to analyze fraud patterns, identify significant features contributing to fraud, and evaluate various machine learning models for fraud detection. By leveraging two distinct datasets, we aim to develop a deep understanding of fraudulent behavior and build predictive models that will aid in identifying and mitigating fraud across different sectors. Specifically, our objectives are as follows:\n",
    "\n",
    "### 1. Fraud Pattern Analysis\n",
    "- **Objective:** Investigate patterns and trends in fraud activities across different sectors, particularly cargo-related fraud and arrest data for fraud cases. This will involve examining how fraudulent activities vary over time and geographic locations, and identifying key factors that influence fraud prevalence.\n",
    "\n",
    "### 2. Feature Importance Assessment\n",
    "- **Objective:** Analyze and rank the importance of different features contributing to fraud detection. By evaluating features such as transaction details, timestamps, geographic data, and socio-economic indicators, we aim to pinpoint the key variables that can most accurately predict fraud occurrences.\n",
    "\n",
    "### 3. Machine Learning Model Development & Evaluation\n",
    "- **Objective:** Develop and compare multiple machine learning models (Random Forest, XGBoost, Logistic Regression) to identify the most effective model for predicting fraud cases. The models will be evaluated on their performance using metrics such as accuracy, precision, recall, and F1-score to ensure robust fraud detection capabilities.\n",
    "\n",
    "### 4. Comprehensive Data Analysis\n",
    "- **Objective:** Through thorough data exploration and analysis, we aim to create a holistic view of fraud activities, utilizing exploratory data analysis (EDA) techniques. This will include visualizing fraud trends, uncovering hidden relationships, and establishing baseline statistics.\n",
    "\n",
    "### 5. Actionable Insights & Final Presentation\n",
    "- **Objective:** By the end of the project, we aim to deliver a comprehensive set of insights that can inform decision-making regarding fraud prevention and detection strategies. These findings will be shared through a final presentation and a dedicated project website hosted via GitHub Pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import requests\n",
    "from io import StringIO\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twelve_month = pd.read_csv(\"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/refs/heads/main/cargo_fraud_only.csv\", index_col=0)\n",
    "data_twelve_month = data_twelve_month.reset_index()\n",
    "data_twelve_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cargo_fraud = data_twelve_month[data_twelve_month[\"offense_name\"].isin([\"Credit Card/Automated Teller Machine Fraud\", \n",
    "                                              \"Wire Fraud\", \n",
    "                                              \"Welfare Fraud\"])]\n",
    "data_cargo_fraud.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of fraud by region\n",
    "fraud_by_state = data_cargo_fraud['region_name'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(fraud_by_state.index, fraud_by_state.values, color='skyblue')\n",
    "plt.title('Proportion of Fraud Occurrences by Region', fontsize=16)\n",
    "plt.xlabel('State', fontsize=12)\n",
    "plt.ylabel('Proportion of Fraud Occurrences', fontsize=12) \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of fraud by state\n",
    "fraud_by_state = data_cargo_fraud['state_name'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# retrieving geopandas usa data\n",
    "usa = gpd.read_file('https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip')\n",
    "\n",
    "# merge the geopandas file and fraud data\n",
    "merged = usa.merge(fraud_by_state, how='left', left_on='NAME', right_on='state_name')\n",
    "merged['proportion'] = merged['proportion'].fillna(0)\n",
    "\n",
    "# exclude Alaska and Hawaii before reprojection\n",
    "merged = merged[~merged['STUSPS'].isin(['AK', 'HI'])]\n",
    "\n",
    "# reproject the GeoDataFrame\n",
    "visframe = merged.to_crs(epsg=2163)\n",
    "\n",
    "# plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "visframe.plot(column='proportion',\n",
    "              cmap='OrRd',\n",
    "              linewidth=0.8,\n",
    "              ax=ax,\n",
    "              edgecolor='0.8',\n",
    "              legend=True,\n",
    "              legend_kwds={\n",
    "                  'label': \"Proportion of Fraud Occurrences by State\",\n",
    "                  'orientation': \"horizontal\"\n",
    "              })\n",
    "\n",
    "ax.set_xlim([-2.5e6, 2.5e6])\n",
    "ax.set_ylim([-2.5e6, 1.5e6])\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Proportion of Fraud Occurrences by State', fontdict={'fontsize': 20}, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of fraud by state\n",
    "fraud_by_state = data_cargo_fraud['location_name'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(fraud_by_state.index, fraud_by_state.values, color='skyblue')\n",
    "plt.title('Proportion of Fraud Occurrences by Location', fontsize=16)\n",
    "plt.xlabel('Location', fontsize=12)\n",
    "plt.ylabel('Proportion of Fraud Occurrences', fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=10, ha='right')  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of fraud by state\n",
    "fraud_by_state = data_cargo_fraud['offender_race'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(fraud_by_state.index, fraud_by_state.values, color='skyblue')\n",
    "plt.title('Proportion of Fraud Occurrences by Race', fontsize=16)\n",
    "plt.xlabel('Race', fontsize=12)\n",
    "plt.ylabel('Proportion of Fraud Occurrences', fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=10, ha='right')  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrences of fraud by victim type\n",
    "fraud_by_type = data_cargo_fraud['victim_type_name'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# plotting the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(fraud_by_type.index, fraud_by_type.values, color='skyblue')\n",
    "plt.xlabel('Victim Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Number of Fraudulent Transactions')\n",
    "plt.title('Proportion of Fraudulent Transactions by Victim Type')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrences of fraud by year\n",
    "fraud_by_year = data_cargo_fraud['data_year'].value_counts().sort_index()\n",
    "\n",
    "# plotting the line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fraud_by_year.index, fraud_by_year.values, color='skyblue', marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Number of Fraudulent Transactions')\n",
    "plt.title('Fraudulent Transactions by Year')\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arrest = pd.read_csv(\"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/refs/heads/main/fraud_onemonth_data.csv\", index_col=0)\n",
    "data_arrest = data_arrest.reset_index()\n",
    "data_arrest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting State Code column to str type\n",
    "data_arrest['State Code'] = data_arrest['State Code'].astype(str).str.strip()\n",
    "\n",
    "# dictionary of state codes to state\n",
    "state_codes = {\n",
    "    '50': 'Alaska',\n",
    "    '01': 'Alabama',\n",
    "    '03': 'Arkansas',\n",
    "    '54': 'American Samoa',\n",
    "    '02': 'Arizona',\n",
    "    '04': 'California',\n",
    "    '05': 'Colorado',\n",
    "    '06': 'Connecticut',\n",
    "    '52': 'Canal Zone',\n",
    "    '08': 'District of Columbia',\n",
    "    '07': 'Delaware',\n",
    "    '09': 'Florida',\n",
    "    '10': 'Georgia',\n",
    "    '55': 'Guam',\n",
    "    '51': 'Hawaii',\n",
    "    '14': 'Iowa',\n",
    "    '11': 'Idaho',\n",
    "    '12': 'Illinois',\n",
    "    '13': 'Indiana',\n",
    "    '15': 'Kansas',\n",
    "    '16': 'Kentucky',\n",
    "    '17': 'Louisiana',\n",
    "    '20': 'Massachusetts',\n",
    "    '19': 'Maryland',\n",
    "    '18': 'Maine',\n",
    "    '21': 'Michigan',\n",
    "    '22': 'Minnesota',\n",
    "    '24': 'Missouri',\n",
    "    '23': 'Mississippi',\n",
    "    '25': 'Montana',\n",
    "    '26': 'Nebraska',\n",
    "    '32': 'North Carolina',\n",
    "    '33': 'North Dakota',\n",
    "    '28': 'New Hampshire',\n",
    "    '29': 'New Jersey',\n",
    "    '30': 'New Mexico',\n",
    "    '27': 'Nevada',\n",
    "    '31': 'New York',\n",
    "    '34': 'Ohio',\n",
    "    '35': 'Oklahoma',\n",
    "    '36': 'Oregon',\n",
    "    '37': 'Pennsylvania',\n",
    "    '53': 'Puerto Rico',\n",
    "    '38': 'Rhode Island',\n",
    "    '39': 'South Carolina',\n",
    "    '40': 'South Dakota',\n",
    "    '41': 'Tennessee',\n",
    "    '42': 'Texas',\n",
    "    '43': 'Utah',\n",
    "    '62': 'Virgin Islands',\n",
    "    '45': 'Virginia',\n",
    "    '44': 'Vermont',\n",
    "    '46': 'Washington',\n",
    "    '48': 'Wisconsin',\n",
    "    '47': 'West Virginia',\n",
    "    '49': 'Wyoming'\n",
    "}\n",
    "\n",
    "# map the state codes to state names\n",
    "data_arrest['State'] = data_arrest['State Code'].map(state_codes)\n",
    "data_arrest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of fraud by state\n",
    "fraud_by_state = data_arrest['State'].value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "# retrieving geopandas usa data\n",
    "usa = gpd.read_file('https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip')\n",
    "\n",
    "# merge the geopandas file and fraud data\n",
    "merged = usa.merge(fraud_by_state, how='left', left_on='NAME', right_on='State')\n",
    "merged['proportion'] = merged['proportion'].fillna(0)\n",
    "\n",
    "# exclude Alaska and Hawaii before reprojection\n",
    "merged = merged[~merged['STUSPS'].isin(['AK', 'HI'])]\n",
    "\n",
    "# reproject the GeoDataFrame\n",
    "visframe = merged.to_crs(epsg=2163)\n",
    "\n",
    "# plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "visframe.plot(column='proportion',\n",
    "              cmap='OrRd',\n",
    "              linewidth=0.8,\n",
    "              ax=ax,\n",
    "              edgecolor='0.8',\n",
    "              legend=True,\n",
    "              legend_kwds={\n",
    "                  'label': \"Proportion of Fraud Occurrences by State\",\n",
    "                  'orientation': \"horizontal\"\n",
    "              })\n",
    "\n",
    "ax.set_xlim([-2.5e6, 2.5e6])\n",
    "ax.set_ylim([-2.5e6, 1.5e6])\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Proportion of Fraud Occurrences by State', fontdict={'fontsize': 20}, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the occurrences of fraud by race\n",
    "fraud_adult_white = data_arrest['Adult White'].sum()\n",
    "fraud_adult_black = data_arrest['Adult Black'].sum()\n",
    "fraud_adult_hispanic = data_arrest['Adult Hispanic'].sum()\n",
    "fraud_adult_indian = data_arrest['Adult Indian'].sum()\n",
    "fraud_adult_asian = data_arrest['Adult Asian'].sum()\n",
    "\n",
    "\n",
    "# combine the counts into a single Series\n",
    "fraud_by_race = pd.Series({\n",
    "    'White': fraud_adult_white,\n",
    "    'Black': fraud_adult_black,\n",
    "    'Hispanic': fraud_adult_hispanic,\n",
    "    'Indian': fraud_adult_indian,\n",
    "    'Asian': fraud_adult_asian\n",
    "})\n",
    "\n",
    "# plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(fraud_by_race.index, fraud_by_race.values, color='skyblue')\n",
    "plt.title('Proportion of Fraud Occurrences by Race', fontsize=16)\n",
    "plt.xlabel('Race', fontsize=12)\n",
    "plt.ylabel('Number of Fraud Occurrences', fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=10, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping U.S. Bureau of Labor Statistics Website for Unemployment Rate data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the response received from scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the payload data\n",
    "payload = {\n",
    "    \"survey\": \"la\",\n",
    "    \"map\": \"state\",\n",
    "    \"seasonal\": \"s\",\n",
    "    \"datatype\": \"unemployment\",\n",
    "    \"year\": \"2012\",\n",
    "    \"period\": \"M12\"\n",
    "}\n",
    "\n",
    "# send the POST request\n",
    "response = requests.post(\"https://data.bls.gov/lausmap/drawMap\", data=payload)\n",
    "\n",
    "# check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # extract the datatableList part\n",
    "    table_data = data.get('tableData')\n",
    "    datatable_list = table_data.get('datatableList')\n",
    "\n",
    "    # print the datatableList\n",
    "    pprint(datatable_list)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise an empty dictionary to store data\n",
    "data_dict = {}\n",
    "\n",
    "# loop over the years from 2012 to 2022\n",
    "for year in range(2012, 2023):\n",
    "    # define the payload data for the endpoint\n",
    "    payload = {\n",
    "        \"survey\": \"la\",\n",
    "        \"map\": \"state\",\n",
    "        \"seasonal\": \"s\",\n",
    "        \"datatype\": \"unemployment\",\n",
    "        \"year\": str(year),\n",
    "        \"period\": \"M12\"\n",
    "    }\n",
    "\n",
    "    # send the POST request\n",
    "    response = requests.post(\"https://data.bls.gov/lausmap/drawMap\", data=payload)\n",
    "\n",
    "    # check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # extract the datatableList part\n",
    "        table_data = data.get('tableData')\n",
    "        datatable_list = table_data.get('datatableList')\n",
    "\n",
    "        # store the data in the dictionary\n",
    "        for entry in datatable_list:\n",
    "            region = entry['displayRegion']\n",
    "            value = entry['currentData']\n",
    "            if region not in data_dict:\n",
    "                data_dict[region] = {}\n",
    "            data_dict[region][year] = value\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for year {year}. Status code: {response.status_code}\")\n",
    "\n",
    "# convert the dictionary to a DataFrame\n",
    "df_unemployment_intermediate = pd.DataFrame(data_dict).T\n",
    "\n",
    "# reset index to convert the DataFrame to a long format\n",
    "df_unemployment_intermediate.reset_index(inplace=True)\n",
    "\n",
    "# rename the index column to 'state'\n",
    "df_unemployment_intermediate.rename(columns={'index': 'state'}, inplace=True)\n",
    "\n",
    "# melt the DataFrame to long format\n",
    "df_unemployment = pd.melt(df_unemployment_intermediate, id_vars=['state'], var_name='year', value_name='unemployment_rate')\n",
    "\n",
    "# convert the 'year' and 'unemployment_rate' column to integer\n",
    "df_unemployment['year'] = df_unemployment['year'].astype(int)\n",
    "df_unemployment['unemployment_rate'] = df_unemployment['unemployment_rate'].astype(float)\n",
    "\n",
    "# display the reshaped DataFrame\n",
    "display(df_unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average unemployment rate by year\n",
    "average_unemployment_by_year = df_unemployment.groupby('year')['unemployment_rate'].mean()\n",
    "\n",
    "# plot the line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_unemployment_by_year.index, average_unemployment_by_year.values, color='skyblue', marker='o')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Unemployment Rate')\n",
    "plt.title('Average Unemployment Rate by Year')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data for the year 2022\n",
    "df_unemployment_2022 = df_unemployment[df_unemployment['year'] == 2022]\n",
    "\n",
    "# retrieving geopandas usa data\n",
    "usa = gpd.read_file('https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip')\n",
    "\n",
    "# merge the geopandas file and unemployment data\n",
    "merged = usa.merge(df_unemployment_2022, how='left', left_on='NAME', right_on='state')\n",
    "merged['unemployment_rate'] = merged['unemployment_rate'].fillna(0)\n",
    "\n",
    "# exclude Alaska and Hawaii before reprojection\n",
    "merged = merged[~merged['STUSPS'].isin(['AK', 'HI'])]\n",
    "\n",
    "# reproject the GeoDataFrame\n",
    "visframe = merged.to_crs(epsg=2163)\n",
    "\n",
    "# plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "visframe.plot(column='unemployment_rate',\n",
    "              cmap='OrRd',\n",
    "              linewidth=0.8,\n",
    "              ax=ax,\n",
    "              edgecolor='0.8',\n",
    "              legend=True,\n",
    "              legend_kwds={\n",
    "                  'label': \"Unemployment Rate by State in 2022\",\n",
    "                  'orientation': \"horizontal\"\n",
    "              })\n",
    "\n",
    "ax.set_xlim([-2.5e6, 2.5e6])\n",
    "ax.set_ylim([-2.5e6, 1.5e6])\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Unemployment Rate by State in 2022', fontdict={'fontsize': 20}, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on 'state' from df_unemployment and 'state_name' from data_cargo\n",
    "# join on 'year' from df_unemployment and 'data_year' from data_cargo\n",
    "merged_df = pd.merge(df_unemployment, data_twelve_month, left_on=['state', 'year'], right_on=['state_name', 'data_year'])\n",
    "\n",
    "merged_df.drop([\"data_year\", \"state_name\"], axis=1, inplace=True)\n",
    "\n",
    "# display the merged DataFrame\n",
    "display(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data_cargo to count the number of entries per state and year\n",
    "cargo_counts = merged_df.groupby(['state', 'year']).agg(\n",
    "    cargo_count=('state', 'size'),  \n",
    "    unemployment_rate=('unemployment_rate', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = cargo_counts['cargo_count'].quantile(0.25)\n",
    "Q3 = cargo_counts['cargo_count'].quantile(0.75)\n",
    "\n",
    "# calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# define the outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# filter out any outliers\n",
    "cargo_counts_filtered = cargo_counts[(cargo_counts['cargo_count'] >= lower_bound) & (cargo_counts['cargo_count'] <= upper_bound)]\n",
    "\n",
    "# plot the correlation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=cargo_counts_filtered, x='unemployment_rate', y='cargo_count', hue='year', palette='tab10', alpha=0.7)\n",
    "plt.xlabel('Unemployment Rate')\n",
    "plt.ylabel('Number of Fraudulent Transactions')\n",
    "plt.title('Correlation between Unemployment Rate and Fraudulent Transactions')\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(cargo_counts_filtered['unemployment_rate'], cargo_counts_filtered['cargo_count']) \n",
    "# create x values for the line\n",
    "x_vals = np.array(cargo_counts_filtered['unemployment_rate'])\n",
    "x_vals_sorted = np.sort(x_vals)\n",
    "y_vals = slope * x_vals_sorted + intercept\n",
    "# plot the regression line\n",
    "plt.plot(x_vals_sorted, y_vals, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the year into periods based on unique years\n",
    "cargo_counts['period'] = pd.cut(cargo_counts['year'], \n",
    "                                bins=[2012, 2013, 2014, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], \n",
    "                                labels=['2012', '2013', '2014', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], right=False)\n",
    "\n",
    "# plotting\n",
    "fig, axes = plt.subplots(2, 5, figsize=(40, 10))\n",
    "\n",
    "period_labels = ['2012', '2013', '2014', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, period in zip(axes, period_labels):\n",
    "    data_period = cargo_counts[cargo_counts['period'] == period]\n",
    "    # calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = data_period['cargo_count'].quantile(0.25)\n",
    "    Q3 = data_period['cargo_count'].quantile(0.75)\n",
    "\n",
    "    # calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # define the outlier boundaries\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # filter out any outliers\n",
    "    data_period = data_period[(data_period['cargo_count'] >= lower_bound) & (data_period['cargo_count'] <= upper_bound)]\n",
    "    sns.scatterplot(x='unemployment_rate', y='cargo_count', data=data_period, ax=ax)\n",
    "\n",
    "    data_period = data_period.dropna(subset=['unemployment_rate', 'cargo_count'])\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(data_period['unemployment_rate'], data_period['cargo_count']) \n",
    "    # create x values for the line\n",
    "    x_vals = np.array(data_period['unemployment_rate'])\n",
    "    x_vals_sorted = np.sort(x_vals)\n",
    "    y_vals = slope * x_vals_sorted + intercept\n",
    "    # plot the regression line\n",
    "    ax.plot(x_vals_sorted, y_vals, color='red')\n",
    "    \n",
    "    ax.set_title(f'Cargo Count vs. Unemployment Rate\\n{period}')\n",
    "    ax.set_xlabel('Unemployment Rate')\n",
    "    ax.set_ylabel('Cargo Count')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cargo fraud data\n",
    "data_twelve_month = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/refs/heads/main/cargo_fraud_only.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "data_twelve_month = data_twelve_month.reset_index()\n",
    "\n",
    "# Filter for specific fraud offenses\n",
    "data_cargo_fraud = data_twelve_month[data_twelve_month[\"offense_name\"].isin([\n",
    "    \"Credit Card/Automated Teller Machine Fraud\", \n",
    "    \"Wire Fraud\", \n",
    "    \"Welfare Fraud\"\n",
    "])]\n",
    "\n",
    "# Count fraudulent transactions by year\n",
    "fraud_by_year = data_cargo_fraud['data_year'].value_counts().sort_index()\n",
    "\n",
    "# Plot fraudulent transactions over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fraud_by_year.index, fraud_by_year.values, marker='o', linestyle='-')\n",
    "plt.title('Fraudulent Transactions by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Fraudulent Transactions: Wire Fraud, Welfare Fraud')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual FRED API key\n",
    "fred = Fred(api_key='9ba8dec145955e0a72bb984873129630')\n",
    "\n",
    "# Fetch CPI data (CPIAUCSL: Consumer Price Index for All Urban Consumers)\n",
    "cpi_data = fred.get_series('CPIAUCSL', observation_start='2008-01-01', observation_end='2022-12-31')\n",
    "\n",
    "# Resample to annual frequency by taking the average\n",
    "cpi_annual = cpi_data.resample('A').mean()\n",
    "\n",
    "# Prepare CPI DataFrame\n",
    "df_cpi = cpi_annual.reset_index(name='CPI')\n",
    "df_cpi['year'] = df_cpi['index'].dt.year\n",
    "df_cpi = df_cpi[['year', 'CPI']]\n",
    "\n",
    "# Display the CPI data\n",
    "display(df_cpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key='9ba8dec145955e0a72bb984873129630')\n",
    "\n",
    "# Fetch the unemployment rate data (UNRATE: Unemployment Rate)\n",
    "unemployment_data = fred.get_series('UNRATE', observation_start='2008-01-01', observation_end='2022-12-31')\n",
    "\n",
    "# Resample to annual frequency by taking the average\n",
    "unemployment_annual = unemployment_data.resample('A').mean()\n",
    "\n",
    "# Prepare Unemployment DataFrame\n",
    "df_unemployment = unemployment_annual.reset_index(name='Unemployment_Rate')\n",
    "df_unemployment['year'] = df_unemployment['index'].dt.year\n",
    "df_unemployment = df_unemployment[['year', 'Unemployment_Rate']]\n",
    "\n",
    "# Display the unemployment data\n",
    "display(df_unemployment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the housing price index data (CSUSHPINSA)\n",
    "housing_data = fred.get_series('CSUSHPINSA', observation_start='2008-01-01', observation_end='2022-12-31')\n",
    "\n",
    "# Resample to annual frequency by taking the average\n",
    "housing_annual = housing_data.resample('A').mean()\n",
    "\n",
    "# Prepare Housing DataFrame\n",
    "df_housing = housing_annual.reset_index(name='Housing_Price_Index')\n",
    "df_housing['year'] = df_housing['index'].dt.year\n",
    "df_housing = df_housing[['year', 'Housing_Price_Index']]\n",
    "\n",
    "# Display the housing price data\n",
    "display(df_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate fraudulent transactions by year\n",
    "fraud_by_year = data_cargo_fraud.groupby('data_year').size().reset_index(name='Fraud_Count')\n",
    "fraud_by_year.rename(columns={'data_year': 'year'}, inplace=True)\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = fraud_by_year.merge(df_cpi, on='year', how='left')\n",
    "merged_data = merged_data.merge(df_unemployment, on='year', how='left')\n",
    "merged_data = merged_data.merge(df_housing, on='year', how='left')\n",
    "\n",
    "# Display the merged data\n",
    "display(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=merged_data, x='year', y='Fraud_Count', marker='o')\n",
    "plt.title('Number of Fraudulent Transactions Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Fraudulent Transactions')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=merged_data, x='year', y='Unemployment_Rate', marker='o', color='green')\n",
    "plt.title('Unemployment Rate Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Unemployment Rate (%)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no missing values\n",
    "merged_data_clean = merged_data.dropna(subset=['Fraud_Count', 'CPI', 'Unemployment_Rate', 'Housing_Price_Index'])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = merged_data_clean[['Fraud_Count', 'CPI', 'Unemployment_Rate', 'Housing_Price_Index']].corr()\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=merged_data, x='Unemployment_Rate', y='Fraud_Count', color='green')\n",
    "sns.regplot(data=merged_data, x='Unemployment_Rate', y='Fraud_Count', scatter=False, color='red')\n",
    "plt.title('Cargo Fraudulent Transactions vs. Unemployment Rate')\n",
    "plt.xlabel('Unemployment Rate (%)')\n",
    "plt.ylabel('Fraudulent Transactions')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FBI Crime Data** :\n",
    "1. The information here is from the FBI Crime Data Explorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Search for datasets related to 'fraud'\n",
    "url = 'https://catalog.data.gov/api/3/action/package_search'\n",
    "params = {\n",
    "    'q': 'fraud',\n",
    "    'rows': 10  # Number of results to return\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data['result']['results']\n",
    "    for dataset in results:\n",
    "        print(f\"Title: {dataset['title']}\")\n",
    "        resource_url = dataset['resources'][0]['url'] if dataset['resources'] else 'No URL available'\n",
    "        print(f\"URL: {resource_url}\\n\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve datasets: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Base API endpoint\n",
    "url = 'https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/'\n",
    "\n",
    "# Define the parameters for the API request\n",
    "params = {\n",
    "    'date_received_min': '2011-12-01',  # Data available from December 2011 onwards\n",
    "    'date_received_max': '2022-12-31',\n",
    "    'issue': ['Identity theft', 'Fraud or scam', 'Unauthorized transactions or other transaction problem'],\n",
    "    'field': [\n",
    "        'date_received',\n",
    "        'product',\n",
    "        'issue',\n",
    "        'company',\n",
    "        'state',\n",
    "        'zip_code',\n",
    "        'company_response',\n",
    "  # Corrected field name\n",
    "        # Removed 'consumer_disputed'\n",
    "    ],\n",
    "    'size': 1000,  # Number of records per page (maximum is 1000)\n",
    "    'no_aggs': True,\n",
    "    'page': 1  # Start with page 1\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store the data\n",
    "fraud_data = pd.DataFrame()\n",
    "\n",
    "# Loop to handle pagination\n",
    "while True:\n",
    "    print(f\"Retrieving page {params['page']}\")\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        hits = data.get('hits', {}).get('hits', [])\n",
    "        if not hits:\n",
    "            break  # No more data\n",
    "        # Extract the data\n",
    "        records = [hit['_source'] for hit in hits]\n",
    "        df_page = pd.DataFrame(records)\n",
    "        fraud_data = pd.concat([fraud_data, df_page], ignore_index=True)\n",
    "        # Move to the next page\n",
    "        params['page'] += 1\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        break\n",
    "\n",
    "# Display the collected data\n",
    "print(f\"Total records retrieved: {len(fraud_data)}\")\n",
    "print(fraud_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_data = pd.read_csv(\"https://raw.githubusercontent.com/ryantangmj/ryantangmj.github.io/refs/heads/main/hpi_by_state.csv\")\n",
    "hpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_data.drop(columns=[\"Abbreviation\", \"FIPS\", \"Annual Change (%)\", \"HPI with 1990 base\", \"HPI with 2000 base\"], inplace=True)\n",
    "hpi_data = hpi_data[(hpi_data[\"Year\"] >= 2012) &  (hpi_data[\"Year\"] <= 2022)]\n",
    "hpi_data.reset_index(drop=True, inplace=True)\n",
    "hpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_by_year = hpi_data.groupby(\"Year\").agg(HPI=('HPI', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Plot HPI over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hpi_by_year.Year, hpi_by_year.HPI, marker='o', linestyle='-')\n",
    "plt.title('Housing Price Index by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Housing Price Index')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_by_state = hpi_data.groupby(\"State\").agg(HPI=('HPI', 'median')\n",
    ").reset_index()\n",
    "\n",
    "# retrieving geopandas usa data\n",
    "usa = gpd.read_file('https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip')\n",
    "\n",
    "# merge the geopandas file and unemployment data\n",
    "merged = usa.merge(hpi_by_state, how='left', left_on='NAME', right_on='State')\n",
    "\n",
    "# exclude Alaska and Hawaii before reprojection\n",
    "merged = merged[~merged['STUSPS'].isin(['AK', 'HI'])]\n",
    "\n",
    "# reproject the GeoDataFrame\n",
    "visframe = merged.to_crs(epsg=2163)\n",
    "\n",
    "# plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "visframe.plot(column='HPI',\n",
    "              cmap='OrRd',\n",
    "              linewidth=0.8,\n",
    "              ax=ax,\n",
    "              edgecolor='0.8',\n",
    "              legend=True,\n",
    "              legend_kwds={\n",
    "                  'label': \"Median Housing Price Index by State from 2012 to 2022\",\n",
    "                  'orientation': \"horizontal\"\n",
    "              })\n",
    "\n",
    "ax.set_xlim([-2.5e6, 2.5e6])\n",
    "ax.set_ylim([-2.5e6, 1.5e6])\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Median Housing Price Index by State from 2012 to 2022', fontdict={'fontsize': 20}, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join on 'State' from df_unemployment and 'state_name' from data_cargo\n",
    "# join on 'Year' from df_unemployment and 'data_year' from data_cargo\n",
    "merge_hpi_fraud_df = pd.merge(hpi_data, data_cargo_fraud, left_on=['State', 'Year'], right_on=['state_name', 'data_year'])\n",
    "\n",
    "merge_hpi_fraud_df.drop([\"data_year\", \"state_name\"], axis=1, inplace=True)\n",
    "\n",
    "# display the merged DataFrame\n",
    "display(merge_hpi_fraud_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate fraudulent transactions by year\n",
    "fraud_by_year_state = merge_hpi_fraud_df.groupby(['Year', 'State', 'HPI']).size().reset_index(name='Fraud_Count')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fraud_by_year_state['HPI'], fraud_by_year_state['Fraud_Count'])\n",
    "plt.title('Scatter Plot of HPI vs Fraud Count')\n",
    "plt.xlabel('Housing Price Index')\n",
    "plt.ylabel('Fraud Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
